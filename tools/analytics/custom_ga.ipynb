{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bdb5804",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: oauth2client in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (4.1.3)\n",
      "Requirement already satisfied: six>=1.6.1 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from oauth2client) (1.16.0)\n",
      "Requirement already satisfied: httplib2>=0.9.1 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from oauth2client) (0.20.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from oauth2client) (0.2.8)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from oauth2client) (0.4.8)\n",
      "Requirement already satisfied: rsa>=3.1.4 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from oauth2client) (4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from httplib2>=0.9.1->oauth2client) (3.0.6)\n",
      "Requirement already satisfied: google-api-python-client in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (2.33.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.0 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from google-api-python-client) (4.1.1)\n",
      "Requirement already satisfied: google-api-core<3.0.0dev,>=1.21.0 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from google-api-python-client) (2.3.0)\n",
      "Requirement already satisfied: google-auth<3.0.0dev,>=1.16.0 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from google-api-python-client) (2.3.3)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.15.0 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from google-api-python-client) (0.20.2)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.1.0 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from google-api-python-client) (0.1.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.52.0 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from google-api-core<3.0.0dev,>=1.21.0->google-api-python-client) (1.54.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from google-api-core<3.0.0dev,>=1.21.0->google-api-python-client) (2.26.0)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from google-api-core<3.0.0dev,>=1.21.0->google-api-python-client) (3.19.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from google-api-core<3.0.0dev,>=1.21.0->google-api-python-client) (58.0.4)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from google-auth<3.0.0dev,>=1.16.0->google-api-python-client) (1.16.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from google-auth<3.0.0dev,>=1.16.0->google-api-python-client) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from google-auth<3.0.0dev,>=1.16.0->google-api-python-client) (4.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from google-auth<3.0.0dev,>=1.16.0->google-api-python-client) (4.2.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from httplib2<1dev,>=0.15.0->google-api-python-client) (3.0.6)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=1.16.0->google-api-python-client) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3.0.0dev,>=1.21.0->google-api-python-client) (2021.5.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3.0.0dev,>=1.21.0->google-api-python-client) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3.0.0dev,>=1.21.0->google-api-python-client) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3.0.0dev,>=1.21.0->google-api-python-client) (2.0.7)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (7.6.5)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from ipywidgets) (5.1.3)\n",
      "Requirement already satisfied: ipython>=4.0.0 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from ipywidgets) (7.16.1)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from ipywidgets) (5.5.5)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from ipywidgets) (1.0.2)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from ipywidgets) (3.5.2)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from ipywidgets) (4.3.3)\n",
      "Requirement already satisfied: tornado>=4.2 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (6.1)\n",
      "Requirement already satisfied: jupyter-client in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (7.1.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (3.0.24)\n",
      "Requirement already satisfied: colorama in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (0.4.4)\n",
      "Requirement already satisfied: jedi>=0.10 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (0.17.2)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: decorator in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (5.1.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (58.0.4)\n",
      "Requirement already satisfied: backcall in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: pygments in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (2.10.0)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from jedi>=0.10->ipython>=4.0.0->ipywidgets) (0.7.1)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from nbformat>=4.2.0->ipywidgets) (4.1.2)\n",
      "Requirement already satisfied: jupyter-core in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from nbformat>=4.2.0->ipywidgets) (4.8.1)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (0.17.3)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (4.8.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (21.2.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: six in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from traitlets>=4.3.1->ipywidgets) (1.16.0)\n",
      "Requirement already satisfied: notebook>=4.4.1 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from widgetsnbextension~=3.5.0->ipywidgets) (6.3.0)\n",
      "Requirement already satisfied: nbconvert in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (6.0.7)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (3.0.3)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.9.5)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.8.0)\n",
      "Requirement already satisfied: argon2-cffi in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (21.1.0)\n",
      "Requirement already satisfied: pyzmq>=17 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (22.3.0)\n",
      "Requirement already satisfied: prometheus-client in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.12.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
      "Requirement already satisfied: nest-asyncio>=1.5 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (1.5.4)\n",
      "Requirement already satisfied: entrypoints in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (0.3)\n",
      "Requirement already satisfied: pywin32>=1.0 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from jupyter-core->nbformat>=4.2.0->ipywidgets) (223)\n",
      "Requirement already satisfied: pywinpty<1,>=0.5 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from terminado>=0.8.3->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.7)\n",
      "Requirement already satisfied: cffi>=1.0.0 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.15.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from cffi>=1.0.0->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.21)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from importlib-metadata->jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (4.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from importlib-metadata->jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (3.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: testpath in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.0)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.7.1)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.9)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.5.0)\n",
      "Requirement already satisfied: bleach in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (4.1.0)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.1.2)\n",
      "Requirement already satisfied: async-generator in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.10)\n",
      "Requirement already satisfied: packaging in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (21.3)\n",
      "Requirement already satisfied: webencodings in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from packaging->bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (3.0.6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: plotly in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (5.4.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from plotly) (8.0.1)\n",
      "Requirement already satisfied: six in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from plotly) (1.16.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (4.62.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from tqdm) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install oauth2client\n",
    "!pip install google-api-python-client\n",
    "!pip install ipywidgets\n",
    "!pip install plotly\n",
    "!pip install tqdm -U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489f8a23",
   "metadata": {},
   "source": [
    "# Pre-requisite installations if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaa555d",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffc55e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import ipywidgets as widgets\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import calendar\n",
    "\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "\n",
    "from apiclient.discovery import build\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "\n",
    "import httplib2\n",
    "from oauth2client import client\n",
    "from oauth2client import file\n",
    "from oauth2client import tools\n",
    "from helper_functions import initialize_analyticsreporting, get_report, print_response, VIEW_ID, next_date_interval, progress_bar_counter\n",
    "\n",
    "pio.renderers.default = \"iframe\"\n",
    "\n",
    "analytics = initialize_analyticsreporting()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3dacd6",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ac9209",
   "metadata": {},
   "source": [
    "#### The cell below will render the widgets needed to select the items in the graph. This cell only needs to be run ONCE (to show the widgets only). After display, you don't have to run this cell. The report/graph will include the end date."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8baf16",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34c223d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67823f7417a446c29bc10d4d5d4bc51a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Option:', options=('New/Returning Users', 'App Launched - OS', 'App Launched - SODA', 'M…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa74c98b55f2432e973a8f500504c1c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "DatePicker(value=None, description='Start Date:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c87c5a9a6c24f2d8508893b9ea96dd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "DatePicker(value=None, description='End Date:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d8216cf8b86420083fc234c3a030eb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Update Interval:', options=('Daily', 'Weekly', 'Monthly', 'No Separation'), value='Daily…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature = widgets.Dropdown(\n",
    "    options=[\n",
    "        'New/Returning Users',\n",
    "        'App Launched - OS', \n",
    "        'App Launched - SODA', \n",
    "        'Manage Dataset - Create Empty Dataset', \n",
    "        'Manage Dataset - Rename Existing Dataset', \n",
    "        'Manage Dataset - Change PI owner', \n",
    "        'Manage Dataset - Add User Permission',\n",
    "        'Manage Dataset - Add/Edit Subtitle',\n",
    "        'Manage Dataset - Add/Edit Description', \n",
    "        'Manage Dataset - Upload Banner Image', \n",
    "        'Manage Dataset - Assign License',\n",
    "        'Manage Dataset - Upload Local Dataset', \n",
    "        'Manage Dataset - Change Dataset Status',\n",
    "        'Manage Datasets - Create a new dataset',\n",
    "        'Manage Datasets - Rename an existing dataset',\n",
    "        'Manage Datasets - Make PI owner of dataset',\n",
    "        'Manage Datasets - Add/Edit Permissions',\n",
    "        'Manage Datasets - Add/Edit Permissions - Add User Permissions',\n",
    "        'Manage Datasets - Add/Edit Permissions - Add Team Permissions',\n",
    "        \"Manage Datasets - Add/Edit Subtitle\",\n",
    "        \"Manage Datasets - Add/Edit Subtitle - Get Subtitle\",\n",
    "        \"Manage Datasets - Add/Edit Readme\",\n",
    "        \"Manage Datasets - Add/Edit Readme - Get Readme\",\n",
    "        \"Manage Datasets - Add/Edit Readme - Parse Readme\",  \n",
    "        \"Manage Datasets - Upload a Banner Image\",\n",
    "        \"Manage Datasets - Upload a Banner Image - Importing Banner Image\",\n",
    "        \"Manage Datasets - Upload a Banner Image - Get Banner Image\",\n",
    "        \"Manage Datasets - Add/Edit Tags\",\n",
    "        \"Manage Datasets - Add/Edit Tags - Get Tags\",\n",
    "        \"Manage Datasets - Assign a License\",\n",
    "        \"Manage Datasets - Assign a License - Get License\",\n",
    "        \"Manage Datasets - Upload Local Dataset\",\n",
    "        \"Manage Datasets - Upload Local Dataset - size\",\n",
    "        \"Manage Datasets - Upload Local Dataset - name - size\",\n",
    "        \"Manage Datasets - Upload Local Dataset - Number of Folders\",\n",
    "        \"Manage Datasets - Upload Local Dataset - name - Number of Folders\",\n",
    "        \"Manage Datasets - Upload Local Dataset - Number of Files\",\n",
    "        \"Manage Datasets - Upload Local Dataset - name - Number of Files\",\n",
    "        \"Manage Datasets - Change Dataset Status\",\n",
    "        \"Manage Datasets - Change Dataset Status - Get Dataset Status\"\n",
    "        \n",
    "        \n",
    "        \n",
    "        'Prepare Metadata - Add Airtable account',\n",
    "        'Prepare Metadata - Add DDD',\n",
    "        'Prepare Metadata - Create Submission',\n",
    "        'Prepare Metadata - Create dataset_description',\n",
    "        'Prepare Metadata - samples',\n",
    "        'Prepare Metadata - samples - Generate',\n",
    "        'Prepare Metadata - samples - Generate - Local',\n",
    "        'Prepare Metadata - samples - Generate - Pennsieve',\n",
    "        'Prepare Metadata - samples - Existing',\n",
    "        'Prepare Metadata - samples - Existing - Local',\n",
    "        'Prepare Metadata - samples - Existing - Pennsieve',\n",
    "        'Prepare Metadata - submission',\n",
    "        'Prepare Metadata - submission - Generate',\n",
    "        'Prepare Metadata - submission - Generate - Local',\n",
    "        'Prepare Metadata - submission - Generate - Pennsieve',\n",
    "        'Prepare Metadata - submission - Existing',\n",
    "        'Prepare Metadata - submission - Existing - Local',\n",
    "        'Prepare Metadata - submission - Existing - Pennsieve',\n",
    "        'Prepare Metadata - dataset_description',\n",
    "        'Prepare Metadata - dataset_description - Generate',\n",
    "        'Prepare Metadata - dataset_description - Generate - Local',\n",
    "        'Prepare Metadata - dataset_description - Generate - Pennsieve',\n",
    "        'Prepare Metadata - dataset_description - Existing',\n",
    "        'Prepare Metadata - dataset_description - Existing - Local',\n",
    "        'Prepare Metadata - dataset_description - Existing - Pennsieve',\n",
    "        'Prepare Metadata - subjects',\n",
    "        'Prepare Metadata - subjects - Generate',\n",
    "        'Prepare Metadata - subjects - Generate - Local',\n",
    "        'Prepare Metadata - subjects - Generate - Pennsieve',\n",
    "        'Prepare Metadata - subjects - Existing',\n",
    "        'Prepare Metadata - subjects - Existing - Local',\n",
    "        'Prepare Metadata - subjects - Existing - Pennsieve',\n",
    "        'Prepare Metadata - readme',\n",
    "        'Prepare Metadata - readme - Generate',\n",
    "        'Prepare Metadata - readme - Generate - Local',\n",
    "        'Prepare Metadata - readme - Generate - Pennsieve',\n",
    "        'Prepare Metadata - readme - Existing',\n",
    "        'Prepare Metadata - readme - Existing - Local',\n",
    "        'Prepare Metadata - readme - Existing - Pennsieve',\n",
    "        'Prepare Metadata - changes',\n",
    "        'Prepare Metadata - changes - Generate',\n",
    "        'Prepare Metadata - changes - Generate - Local',\n",
    "        'Prepare Metadata - changes - Generate - Pennsieve',\n",
    "        'Prepare Metadata - changes - Existing',\n",
    "        'Prepare Metadata - changes - Existing - Local',\n",
    "        'Prepare Metadata - changes - Existing - Pennsieve',\n",
    "        'Prepare Metadata - manifest',\n",
    "        'Prepare Metadata - manifest - Generate',\n",
    "        'Prepare Metadata - manifest - Generate - Local',\n",
    "        'Prepare Metadata - manifest - Generate - Pennsieve',\n",
    "        'Prepare Metadata - manifest - Existing',\n",
    "        'Prepare Metadata - manifest - Existing - Local',\n",
    "        'Prepare Metadata - manifest - Existing - Pennsieve',\n",
    "        \n",
    "        \n",
    "        'Generate Dataset',\n",
    "        'Generate Dataset - Local',\n",
    "        'Generate Dataset - Blackfynn',\n",
    "        'Generate Dataset - Pennsieve',\n",
    "        \n",
    "        \n",
    "        'Manifest Files Created',\n",
    "        'Manifest Files Created - Blackfynn', \n",
    "        'Manifest Files Created - Pennsieve', \n",
    "        'Manifest Files Created - Local',\n",
    "        \n",
    "        \n",
    "        'Download Template - manifest.xlsx',\n",
    "        'Download Template - manifest.xlsx',\n",
    "        'Download Template - dataset_description.xlsx',\n",
    "        'Download Template - subjects.xlsx',\n",
    "        'Download Template - samples.xlsx',\n",
    "        'Download Template - submission.xlsx',\n",
    "        \n",
    "        \n",
    "        'Disseminate Dataset - Share with Curation Team', \n",
    "        'Disseminate Dataset - Share with Consortium',\n",
    "        'Disseminate Dataset - Pre-publishing Review',\n",
    "        \n",
    "        \n",
    "        'Prepare Datasets - Organize dataset',\n",
    "        'Prepare Datasets - Organize dataset - Existing',\n",
    "        'Prepare Datasets - Organize dataset - Existing - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Existing - Local',\n",
    "        'Prepare Datasets - Organize dataset - Existing - Saved',\n",
    "        \n",
    "        \n",
    "        'Prepare Datasets - Organize dataset - Step 3',\n",
    "        'Prepare Datasets - Organize dataset - Step 3 - Import',\n",
    "        'Prepare Datasets - Organize dataset - Step 3 - Import - File',\n",
    "        'Prepare Datasets - Organize dataset - Step 3 - Import - File - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 3 - Import - File - Saved',\n",
    "        'Prepare Datasets - Organize dataset - Step 3 - Import - File - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 3 - Import - File - New',\n",
    "        \n",
    "        'Prepare Datasets - Organize dataset - Step 3 - Import - Folder',\n",
    "        'Prepare Datasets - Organize dataset - Step 3 - Import - Folder - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 3 - Import - Folder - Saved',\n",
    "        'Prepare Datasets - Organize dataset - Step 3 - Import - Folder - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 3 - Import - Folder - New',\n",
    "        \n",
    "        'Prepare Datasets - Organize dataset - Step 3 - Add - Folder',\n",
    "        'Prepare Datasets - Organize dataset - Step 3 - Add - Folder - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 3 - Add - Folder - Saved',\n",
    "        'Prepare Datasets - Organize dataset - Step 3 - Add - Folder - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 3 - Add - Folder - New',\n",
    "        \n",
    "        \n",
    "        'Prepare Datasets - Organize dataset - Step 4',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - subjects - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - subjects - Local - Saved',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - subjects - Local - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - subjects - Local - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - subjects - Local - New',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - subjects - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - subjects - Pennsieve - Saved',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - subjects - Pennsieve - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - subjects - Pennsieve - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - subjects - Pennsieve - New',\n",
    "        \n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - samples - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - samples - Local - Saved',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - samples - Local - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - samples - Local - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - samples - Local - New',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - samples - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - samples - Pennsieve - Saved',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - samples - Pennsieve - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - samples - Pennsieve - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - samples - Pennsieve - New',\n",
    "        \n",
    "        \n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - submission - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - submission - Local - Saved',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - submission - Local - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - submission - Local - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - submission - Local - New',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - submission - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - submission - Pennsieve - Saved',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - submission - Pennsieve - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - submission - Pennsieve - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - submission - Pennsieve - New',\n",
    "        \n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - dataset_description - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - dataset_description - Local - Saved',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - dataset_description - Local - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - dataset_description - Local - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - dataset_description - Local - New',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - dataset_description - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - dataset_description - Pennsieve - Saved',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - dataset_description - Pennsieve - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - dataset_description - Pennsieve - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - dataset_description - Pennsieve - New',\n",
    "        \n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - README - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - README - Local - Saved',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - README - Local - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - README - Local - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - README - Local - New',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - README - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - README - Pennsieve - Saved',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - README - Pennsieve - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - README - Pennsieve - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - README - Pennsieve - New',\n",
    "        \n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - CHANGES - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - CHANGES - Local - Saved',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - CHANGES - Local - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - CHANGES - Local - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - CHANGES - Local - New',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - CHANGES - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - CHANGES - Pennsieve - Saved',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - CHANGES - Pennsieve - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - CHANGES - Pennsieve - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - CHANGES - Pennsieve - New',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - CHANGES - Pennsieve',\n",
    "        \n",
    "        \n",
    "        'Prepare Datasets - Organize dataset - Step 7',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Pennsieve - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Pennsieve - Saved',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Pennsieve - New',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Pennsieve - Pennsieve',\n",
    "        \n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Local - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Local - Saved',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Local - New',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Local - Pennsieve',\n",
    "        \n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Size',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Pennsieve - Size',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Local - Size',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Number of Files',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Pennsieve - Number of Files',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Local - Number of Files',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Manifest',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Manifest - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Manifest - Local',\n",
    "        \n",
    "        \n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Pennsieve - Create a duplicate',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Pennsieve - Replace',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Pennsieve - Merge',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Pennsieve - Skip',\n",
    "        \n",
    "        'Disseminate Datasets - Show current dataset permission',\n",
    "        'Disseminate Datasets - Show current dataset status',\n",
    "        'Disseminate Datasets - Pre-publishing Review - Integrate ORCID iD',\n",
    "        'Disseminate Datasets - Pre-publishing Review - Get Excluded Files',\n",
    "        'Disseminate Datasets - Pre-publishing Review - Get Metadata Files',\n",
    "        'Disseminate Datasets - Pre-publishing Review - Update excluded files',\n",
    "        'Disseminate Datasets - Pre-publishing Review - Publish',\n",
    "        'Disseminate Datasets - Pre-publishing Review - Submit dataset',\n",
    "        'Disseminate Datasets - Pre-publishing Review - Withdraw dataset',\n",
    "        'Disseminate Datasets - Pre-publishing Review - Fetch Pre-publishing Checklist Statuses',\n",
    "        \"Disseminate Datasets - Pre-publishing Review - Determine User's Dataset Role\",\n",
    "        \n",
    "        'Disseminate Datasets - Share with Curation Team',\n",
    "        \"Disseminate Datasets - Share with Curation Team - Remove Consortium's Team Permissions\",\n",
    "        \"Disseminate Datasets - Share with Curation Team - Give Consortium Team Permissions\",\n",
    "        \"Disseminate Datasets - Share with Curation Team - Change Dataset Status to Work In Progress\",\n",
    "        \"Disseminate Datasets - Share with Curation Team - Change Dataset Status to Ready for Curation\",\n",
    "        \n",
    "        'Disseminate Datasets - Share with Consortium',\n",
    "        'Disseminate Datasets - Share with Consortium - Removed Team Permissions SPARC Consortium',\n",
    "        'Disseminate Datasets - Share with Consortium - Add Team Permissions SPARC Consortium',\n",
    "        'Disseminate Datasets - Share with Consortium - Curated & Awaiting PI Approval',\n",
    "        'Disseminate Datasets - Share with Consortium - Change Dataset Status to Under Embargo'\n",
    "        'Disseminate Datasets - Pre-publishing Review',\n",
    "        ],\n",
    "    value='New/Returning Users',\n",
    "    description='Option:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "start_date = widgets.DatePicker(description='Start Date:', disabled=False)\n",
    "end_date = widgets.DatePicker(description='End Date:', disabled=False)\n",
    "\n",
    "update_interval = widgets.Dropdown(options=['Daily', 'Weekly', 'Monthly', \"No Separation\"], description='Update Interval:', disabled=False)\n",
    "\n",
    "display(feature, start_date, end_date, update_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1313ce9f",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe6fb62",
   "metadata": {},
   "source": [
    "#### The cell below is a basic function that uses the widgets in the cell above to create a graph. If the widgets are not showing, run the widget cell. You don't have to run it again after selecting a value. Changing the value of the dropdown will dynamically change the value of the variable in the next cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80055def",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a73189d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75cfc6c2a0ea45229f82a40effcc5b6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ga:eventAction:  App Launched - OS\n",
      "Date range: 0\n",
      "ga:totalEvents: 83\n",
      "ga:eventAction:  App Launched - SODA\n",
      "Date range: 0\n",
      "ga:totalEvents: 83\n",
      "ga:eventAction:  Disseminate Datasets - Pre-publishing Review - Fetch Pre-publishing Checklist Statuses\n",
      "Date range: 0\n",
      "ga:totalEvents: 5\n",
      "ga:eventAction:  Establishing Python Connection\n",
      "Date range: 0\n",
      "ga:totalEvents: 211\n",
      "ga:eventAction:  Generate Dataset\n",
      "Date range: 0\n",
      "ga:totalEvents: 4\n",
      "ga:eventAction:  Generate Dataset -\n",
      "Date range: 0\n",
      "ga:totalEvents: 4\n",
      "ga:eventAction:  Generate Dataset - - Number of Files\n",
      "Date range: 0\n",
      "ga:totalEvents: 4\n",
      "ga:eventAction:  Generate Dataset - - Size\n",
      "Date range: 0\n",
      "ga:totalEvents: 4\n",
      "ga:eventAction:  Generate Dataset - Number of Files\n",
      "Date range: 0\n",
      "ga:totalEvents: 4\n",
      "ga:eventAction:  Generate Dataset - Size\n",
      "Date range: 0\n",
      "ga:totalEvents: 4\n",
      "ga:eventAction:  Manage Dataset - Create Empty Dataset\n",
      "Date range: 0\n",
      "ga:totalEvents: 2\n",
      "ga:eventAction:  Manage Datasets - Add/Edit Readme\n",
      "Date range: 0\n",
      "ga:totalEvents: 1\n",
      "ga:eventAction:  Manage Datasets - Add/Edit Readme - Get readme\n",
      "Date range: 0\n",
      "ga:totalEvents: 32\n",
      "ga:eventAction:  Manage Datasets - Add/Edit Readme - Parse readme\n",
      "Date range: 0\n",
      "ga:totalEvents: 32\n",
      "ga:eventAction:  Manage Datasets - Add/Edit Subtitle\n",
      "Date range: 0\n",
      "ga:totalEvents: 1\n",
      "ga:eventAction:  Manage Datasets - Add/Edit Subtitle - Get Subtitle\n",
      "Date range: 0\n",
      "ga:totalEvents: 32\n",
      "ga:eventAction:  Manage Datasets - Add/Edit tags - Get Tags\n",
      "Date range: 0\n",
      "ga:totalEvents: 1\n",
      "ga:eventAction:  Manage Datasets - Assign a license\n",
      "Date range: 0\n",
      "ga:totalEvents: 1\n",
      "ga:eventAction:  Manage Datasets - Change Dataset Status\n",
      "Date range: 0\n",
      "ga:totalEvents: 1\n",
      "ga:eventAction:  Manage Datasets - Change Dataset Status - Get dataset status\n",
      "Date range: 0\n",
      "ga:totalEvents: 32\n",
      "ga:eventAction:  Manage Datasets - Create a new dataset\n",
      "Date range: 0\n",
      "ga:totalEvents: 9\n",
      "ga:eventAction:  Manage Datasets - Make PI owner of dataset\n",
      "Date range: 0\n",
      "ga:totalEvents: 1\n",
      "ga:eventAction:  Manage Datasets - Rename an existing dataset\n",
      "Date range: 0\n",
      "ga:totalEvents: 3\n",
      "ga:eventAction:  Manage Datasets - Upload a banner image\n",
      "Date range: 0\n",
      "ga:totalEvents: 1\n",
      "ga:eventAction:  Manage Datasets - Upload a banner image - Get Banner Image\n",
      "Date range: 0\n",
      "ga:totalEvents: 33\n",
      "ga:eventAction:  Manage Datasets - Upload Local Dataset\n",
      "Date range: 0\n",
      "ga:totalEvents: 4\n",
      "ga:eventAction:  Manage Datasets - Upload local dataset - name - Number of Files\n",
      "Date range: 0\n",
      "ga:totalEvents: 4\n",
      "ga:eventAction:  Manage Datasets - Upload local dataset - name - Number of Folders\n",
      "Date range: 0\n",
      "ga:totalEvents: 4\n",
      "ga:eventAction:  Manage Datasets - Upload local dataset - name - size\n",
      "Date range: 0\n",
      "ga:totalEvents: 4\n",
      "ga:eventAction:  Manage Datasets - Upload local dataset - Number of Files\n",
      "Date range: 0\n",
      "ga:totalEvents: 4\n",
      "ga:eventAction:  Manage Datasets - Upload local dataset - Number of Folders\n",
      "Date range: 0\n",
      "ga:totalEvents: 4\n",
      "ga:eventAction:  Manage Datasets - Upload local dataset - Progress track\n",
      "Date range: 0\n",
      "ga:totalEvents: 94\n",
      "ga:eventAction:  Manage Datasets - Upload local dataset - size\n",
      "Date range: 0\n",
      "ga:totalEvents: 4\n",
      "ga:eventAction:  Manage Datasets Add/edit permissions\n",
      "Date range: 0\n",
      "ga:totalEvents: 1\n",
      "ga:eventAction:  Manage Datasets Add/edit permissions - Add User Permissions\n",
      "Date range: 0\n",
      "ga:totalEvents: 1\n",
      "ga:eventAction:  Prepare Datasets - Organize dataset\n",
      "Date range: 0\n",
      "ga:totalEvents: 9\n",
      "ga:eventAction:  Prepare Datasets - Organize dataset - New\n",
      "Date range: 0\n",
      "ga:totalEvents: 9\n",
      "ga:eventAction:  Prepare Datasets - Organize dataset - New - undefined\n",
      "Date range: 0\n",
      "ga:totalEvents: 9\n",
      "ga:eventAction:  Prepare Metadata - changes\n",
      "Date range: 0\n",
      "ga:totalEvents: 4\n",
      "ga:eventAction:  Prepare Metadata - changes - Existing\n",
      "Date range: 0\n",
      "ga:totalEvents: 1\n",
      "ga:eventAction:  Prepare Metadata - changes - Existing - Pennsieve\n",
      "Date range: 0\n",
      "ga:totalEvents: 1\n",
      "ga:eventAction:  Prepare Metadata - changes - Generate\n",
      "Date range: 0\n",
      "ga:totalEvents: 4\n",
      "ga:eventAction:  Prepare Metadata - changes - Generate - Local\n",
      "Date range: 0\n",
      "ga:totalEvents: 1\n",
      "ga:eventAction:  Prepare Metadata - changes - Generate - Pennsieve\n",
      "Date range: 0\n",
      "ga:totalEvents: 1\n",
      "ga:eventAction:  Prepare Metadata - Continue with existing dataset_description.xlsx\n",
      "Date range: 0\n",
      "ga:totalEvents: 1\n",
      "ga:eventAction:  Prepare Metadata - Continue with existing subjects.xlsx\n",
      "Date range: 0\n",
      "ga:totalEvents: 1\n",
      "ga:eventAction:  Prepare Metadata - dataset_description\n",
      "Date range: 0\n",
      "ga:totalEvents: 4\n",
      "ga:eventAction:  Prepare Metadata - dataset_description - Existing\n",
      "Date range: 0\n",
      "ga:totalEvents: 2\n",
      "ga:eventAction:  Prepare Metadata - dataset_description - Existing - Local\n",
      "Date range: 0\n",
      "ga:totalEvents: 1\n",
      "ga:eventAction:  Prepare Metadata - dataset_description - Existing - Pennsieve\n",
      "Date range: 0\n",
      "ga:totalEvents: 1\n",
      "ga:eventAction:  Prepare Metadata - dataset_description - Generate\n",
      "Date range: 0\n",
      "ga:totalEvents: 3\n",
      "ga:eventAction:  Prepare Metadata - dataset_description - Generate - Local\n",
      "Date range: 0\n",
      "ga:totalEvents: 1\n",
      "ga:eventAction:  Prepare Metadata - dataset_description - Generate - Pennsieve\n",
      "Date range: 0\n",
      "ga:totalEvents: 1\n",
      "ga:eventAction:  Prepare Metadata - manifest\n",
      "Date range: 0\n",
      "ga:totalEvents: 4\n",
      "ga:eventAction:  Prepare Metadata - manifest - Generate\n",
      "Date range: 0\n",
      "ga:totalEvents: 4\n",
      "ga:eventAction:  Prepare Metadata - manifest - Generate - Local\n",
      "Date range: 0\n",
      "ga:totalEvents: 3\n",
      "ga:eventAction:  Prepare Metadata - manifest - Generate - Pennsieve\n",
      "Date range: 0\n",
      "ga:totalEvents: 1\n",
      "ga:eventAction:  Prepare Metadata - readme\n",
      "Date range: 0\n",
      "ga:totalEvents: 3\n",
      "ga:eventAction:  Prepare Metadata - readme - Existing\n",
      "Date range: 0\n",
      "ga:totalEvents: 3\n",
      "ga:eventAction:  Prepare Metadata - readme - Existing - Local\n",
      "Date range: 0\n",
      "ga:totalEvents: 2\n",
      "ga:eventAction:  Prepare Metadata - readme - Existing - Pennsieve\n",
      "Date range: 0\n",
      "ga:totalEvents: 1\n",
      "ga:eventAction:  Prepare Metadata - readme - Generate\n",
      "Date range: 0\n",
      "ga:totalEvents: 3\n",
      "ga:eventAction:  Prepare Metadata - readme - Generate - Local\n",
      "Date range: 0\n",
      "ga:totalEvents: 1\n",
      "ga:eventAction:  Prepare Metadata - readme - Generate - Pennsieve\n",
      "Date range: 0\n",
      "ga:totalEvents: 1\n",
      "ga:eventAction:  Prepare Metadata - samples\n",
      "Date range: 0\n",
      "ga:totalEvents: 10\n",
      "ga:eventAction:  Prepare Metadata - samples - Existing\n",
      "Date range: 0\n",
      "ga:totalEvents: 10\n",
      "ga:eventAction:  Prepare Metadata - samples - Existing - Local\n",
      "Date range: 0\n",
      "ga:totalEvents: 3\n",
      "ga:eventAction:  Prepare Metadata - samples - Existing - Pennsieve\n",
      "Date range: 0\n",
      "ga:totalEvents: 2\n",
      "ga:eventAction:  Prepare Metadata - samples - Generate\n",
      "Date range: 0\n",
      "ga:totalEvents: 9\n",
      "ga:eventAction:  Prepare Metadata - samples - Generate - Local\n",
      "Date range: 0\n",
      "ga:totalEvents: 1\n",
      "ga:eventAction:  Prepare Metadata - samples - Generate - Pennsieve\n",
      "Date range: 0\n",
      "ga:totalEvents: 4\n",
      "ga:eventAction:  Prepare Metadata - subjects\n",
      "Date range: 0\n",
      "ga:totalEvents: 3\n",
      "ga:eventAction:  Prepare Metadata - subjects - Existing\n",
      "Date range: 0\n",
      "ga:totalEvents: 2\n",
      "ga:eventAction:  Prepare Metadata - subjects - Existing - Local\n",
      "Date range: 0\n",
      "ga:totalEvents: 1\n",
      "ga:eventAction:  Prepare Metadata - subjects - Existing - Pennsieve\n",
      "Date range: 0\n",
      "ga:totalEvents: 1\n",
      "ga:eventAction:  Prepare Metadata - subjects - Generate\n",
      "Date range: 0\n",
      "ga:totalEvents: 3\n",
      "ga:eventAction:  Prepare Metadata - subjects - Generate - Local\n",
      "Date range: 0\n",
      "ga:totalEvents: 1\n",
      "ga:eventAction:  Prepare Metadata - subjects - Generate - Pennsieve\n",
      "Date range: 0\n",
      "ga:totalEvents: 1\n",
      "ga:eventAction:  Prepare Metadata - submission\n",
      "Date range: 0\n",
      "ga:totalEvents: 6\n",
      "ga:eventAction:  Prepare Metadata - submission - Existing\n",
      "Date range: 0\n",
      "ga:totalEvents: 3\n",
      "ga:eventAction:  Prepare Metadata - submission - Existing - Local\n",
      "Date range: 0\n",
      "ga:totalEvents: 1\n",
      "ga:eventAction:  Prepare Metadata - submission - Existing - Pennsieve\n",
      "Date range: 0\n",
      "ga:totalEvents: 1\n",
      "ga:eventAction:  Prepare Metadata - submission - Generate\n",
      "Date range: 0\n",
      "ga:totalEvents: 6\n",
      "ga:eventAction:  Prepare Metadata - submission - Generate - Local\n",
      "Date range: 0\n",
      "ga:totalEvents: 3\n",
      "ga:eventAction:  Prepare Metadata - submission - Generate - Pennsieve\n",
      "Date range: 0\n",
      "ga:totalEvents: 1\n",
      "ga:eventAction:  Retrieve Dataset - Pennsieve\n",
      "Date range: 0\n",
      "ga:totalEvents: 1\n",
      "ga:eventAction:  Verifying App Version\n",
      "Date range: 0\n",
      "ga:totalEvents: 196\n",
      "ga:eventAction:  App Launched - OS\n",
      "Date range: 0\n",
      "ga:totalEvents: 2\n",
      "ga:eventAction:  App Launched - SODA\n",
      "Date range: 0\n",
      "ga:totalEvents: 2\n",
      "ga:eventAction:  Disseminate Datasets - Pre-publishing Review - Determine User's Dataset Role\n",
      "Date range: 0\n",
      "ga:totalEvents: 1\n",
      "ga:eventAction:  Disseminate Datasets - Pre-publishing Review - Fetch Pre-publishing Checklist Statuses\n",
      "Date range: 0\n",
      "ga:totalEvents: 2\n",
      "ga:eventAction:  Disseminate Datasets - Share with Curation Team\n",
      "Date range: 0\n",
      "ga:totalEvents: 4\n",
      "ga:eventAction:  Disseminate Datasets - Share with Curation Team - Change Dataset Status to Ready for Curation\n",
      "Date range: 0\n",
      "ga:totalEvents: 2\n",
      "ga:eventAction:  Disseminate Datasets - Share with Curation Team - Change Dataset Status to Work In Progress\n",
      "Date range: 0\n",
      "ga:totalEvents: 2\n",
      "ga:eventAction:  Disseminate Datasets - Share with Curation Team - Give Consortium Team Permissions\n",
      "Date range: 0\n",
      "ga:totalEvents: 3\n",
      "ga:eventAction:  Disseminate Datasets - Share with Curation Team - Remove Consortium's Team Permissions\n",
      "Date range: 0\n",
      "ga:totalEvents: 1\n",
      "ga:eventAction:  Disseminate Datasets - Share with SPARC Consortium\n",
      "Date range: 0\n",
      "ga:totalEvents: 2\n",
      "ga:eventAction:  Disseminate Datasets - Share with SPARC Consortium - Add Team Permissions SPARC Consortium\n",
      "Date range: 0\n",
      "ga:totalEvents: 1\n",
      "ga:eventAction:  Disseminate Datasets - Share with SPARC Consortium - Change Dataset Status to Under Embargo\n",
      "Date range: 0\n",
      "ga:totalEvents: 1\n",
      "ga:eventAction:  Disseminate Datasets - Share with SPARC Consortium - Curated & Awaiting PI Approval\n",
      "Date range: 0\n",
      "ga:totalEvents: 1\n",
      "ga:eventAction:  Disseminate Datasets - Share with SPARC Consortium - Removed Team Permissions SPARC Consortium\n",
      "Date range: 0\n",
      "ga:totalEvents: 1\n",
      "ga:eventAction:  Disseminate Datasets - Show current dataset permission\n",
      "Date range: 0\n",
      "ga:totalEvents: 6\n",
      "ga:eventAction:  Disseminate Datasets - Show current dataset status\n",
      "Date range: 0\n",
      "ga:totalEvents: 6\n",
      "ga:eventAction:  Establishing Python Connection\n",
      "Date range: 0\n",
      "ga:totalEvents: 13\n",
      "ga:eventAction:  Generate Dataset\n",
      "Date range: 0\n",
      "ga:totalEvents: 4\n",
      "ga:eventAction:  Generate Dataset -\n",
      "Date range: 0\n",
      "ga:totalEvents: 4\n",
      "ga:eventAction:  Generate Dataset - - Number of Files\n",
      "Date range: 0\n",
      "ga:totalEvents: 4\n",
      "ga:eventAction:  Generate Dataset - - Size\n",
      "Date range: 0\n",
      "ga:totalEvents: 4\n",
      "ga:eventAction:  Generate Dataset - Number of Files\n",
      "Date range: 0\n",
      "ga:totalEvents: 4\n",
      "ga:eventAction:  Generate Dataset - Size\n",
      "Date range: 0\n",
      "ga:totalEvents: 4\n",
      "ga:eventAction:  Manage Datasets - Add/Edit Readme - Get readme\n",
      "Date range: 0\n",
      "ga:totalEvents: 2\n",
      "ga:eventAction:  Manage Datasets - Add/Edit Readme - Parse readme\n",
      "Date range: 0\n",
      "ga:totalEvents: 2\n",
      "ga:eventAction:  Manage Datasets - Add/Edit Subtitle - Get Subtitle\n",
      "Date range: 0\n",
      "ga:totalEvents: 2\n",
      "ga:eventAction:  Manage Datasets - Assign a license\n",
      "Date range: 0\n",
      "ga:totalEvents: 1\n",
      "ga:eventAction:  Manage Datasets - Change Dataset Status - Get dataset status\n",
      "Date range: 0\n",
      "ga:totalEvents: 8\n",
      "ga:eventAction:  Manage Datasets - Upload a banner image - Get Banner Image\n",
      "Date range: 0\n",
      "ga:totalEvents: 2\n",
      "ga:eventAction:  Prepare Datasets - Organize dataset - Existing\n",
      "Date range: 0\n",
      "ga:totalEvents: 3\n",
      "ga:eventAction:  Prepare Datasets - Organize dataset - Existing - Saved\n",
      "Date range: 0\n",
      "ga:totalEvents: 1\n",
      "ga:eventAction:  Prepare Datasets - Organize dataset - New\n",
      "Date range: 0\n",
      "ga:totalEvents: 4\n",
      "ga:eventAction:  Prepare Datasets - Organize dataset - Step 3\n",
      "Date range: 0\n",
      "ga:totalEvents: 3\n",
      "ga:eventAction:  Prepare Datasets - Organize dataset - Step 3 - Add\n",
      "Date range: 0\n",
      "ga:totalEvents: 1\n",
      "ga:eventAction:  Prepare Datasets - Organize dataset - Step 3 - Add - Folder\n",
      "Date range: 0\n",
      "ga:totalEvents: 1\n",
      "ga:eventAction:  Prepare Datasets - Organize dataset - Step 3 - Add - Folder - Pennsieve\n",
      "Date range: 0\n",
      "ga:totalEvents: 1\n",
      "ga:eventAction:  Prepare Datasets - Organize dataset - Step 3 - Import\n",
      "Date range: 0\n",
      "ga:totalEvents: 2\n",
      "ga:eventAction:  Prepare Datasets - Organize dataset - Step 3 - Import - File\n",
      "Date range: 0\n",
      "ga:totalEvents: 1\n",
      "ga:eventAction:  Prepare Datasets - Organize dataset - Step 3 - Import - File - Pennsieve\n",
      "Date range: 0\n",
      "ga:totalEvents: 1\n",
      "ga:eventAction:  Prepare Datasets - Organize dataset - Step 3 - Import - Folder\n",
      "Date range: 0\n",
      "ga:totalEvents: 1\n",
      "ga:eventAction:  Prepare Datasets - Organize dataset - Step 3 - Import - Folder - Pennsieve\n",
      "Date range: 0\n",
      "ga:totalEvents: 1\n",
      "ga:eventAction:  Prepare Datasets - Organize dataset - Step 4\n",
      "Date range: 0\n",
      "ga:totalEvents: 2\n",
      "ga:eventAction:  Prepare Datasets - Organize dataset - Step 4 - Import\n",
      "Date range: 0\n",
      "ga:totalEvents: 2\n",
      "ga:eventAction:  Prepare Datasets - Organize dataset - Step 4 - Import - submission\n",
      "Date range: 0\n",
      "ga:totalEvents: 2\n",
      "ga:eventAction:  Prepare Datasets - Organize dataset - Step 4 - Import - submission - Local\n",
      "Date range: 0\n",
      "ga:totalEvents: 1\n",
      "ga:eventAction:  Prepare Datasets - Organize dataset - Step 4 - Import - submission - Local - Pennsieve\n",
      "Date range: 0\n",
      "ga:totalEvents: 1\n",
      "ga:eventAction:  Prepare Datasets - Organize dataset - Step 4 - Import - submission - Pennsieve\n",
      "Date range: 0\n",
      "ga:totalEvents: 1\n",
      "ga:eventAction:  Prepare Datasets - Organize dataset - Step 4 - Import - submission - Pennsieve - Pennsieve\n",
      "Date range: 0\n",
      "ga:totalEvents: 1\n",
      "ga:eventAction:  Retrieve Dataset - Pennsieve\n",
      "Date range: 0\n",
      "ga:totalEvents: 1\n",
      "ga:eventAction:  Verifying App Version\n",
      "Date range: 0\n",
      "ga:totalEvents: 13\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_4.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dt = start_date.value\n",
    "ds = end_date.value\n",
    "\n",
    "data, new_user_data, returning_user_data = [], [], []\n",
    "column_headers = []\n",
    "file_name = \"\"\n",
    "bar_counter = 0\n",
    "\n",
    "if update_interval.value == \"Daily\":\n",
    "    bar_counter = progress_bar_counter(dt, ds, \"Daily\")\n",
    "    start = end = dt\n",
    "    column_headers = ['Day', 'Frequency']\n",
    "    file_name = \"daily\"\n",
    "if update_interval.value == \"Weekly\":\n",
    "    bar_counter = progress_bar_counter(dt, ds, \"Weekly\")    \n",
    "    start = dt - timedelta(days=dt.weekday())\n",
    "    end = start + timedelta(days=6)\n",
    "    column_headers = ['Week', 'Frequency']\n",
    "    file_name = \"weekly\"\n",
    "if update_interval.value == \"Monthly\":\n",
    "    bar_counter = progress_bar_counter(dt, ds, \"Monthly\")\n",
    "    start = end = dt\n",
    "    end = end.replace(day = calendar.monthrange(start.year, start.month)[1])\n",
    "    column_headers = ['Month', 'Frequency']\n",
    "    file_name = \"monthly\"\n",
    "if update_interval.value == \"No Separation\":\n",
    "    bar_counter = 1\n",
    "    start = dt\n",
    "    end = ds\n",
    "    column_headers = ['Time Period', 'Frequency']\n",
    "    file_name = \"no_Separation\"\n",
    "    \n",
    "for i in trange(bar_counter):\n",
    "    if start <= ds:\n",
    "        if feature.value == \"New/Returning Users\":\n",
    "            query = {\n",
    "                'reportRequests': [\n",
    "                {\n",
    "                    'viewId': VIEW_ID,\n",
    "                    'dateRanges': [{'startDate': start.strftime('%Y-%m-%d'), 'endDate': end.strftime('%Y-%m-%d')}],\n",
    "                    'metrics': [{'expression': 'ga:users'}],\n",
    "                    'dimensions': [{'name': 'ga:userType'}]\n",
    "                }]\n",
    "            }\n",
    "        else:\n",
    "            query = {\n",
    "                'reportRequests': [\n",
    "                {\n",
    "                    'viewId': VIEW_ID,\n",
    "                    'dateRanges': [{'startDate': start.strftime('%Y-%m-%d'), 'endDate': end.strftime('%Y-%m-%d')}],\n",
    "                    'metrics': [{'expression': 'ga:totalEvents'}],\n",
    "                    'dimensions': [{'name': 'ga:eventAction'}]\n",
    "                }]\n",
    "            }\n",
    "            \n",
    "        \n",
    "            \n",
    "        cell_data, new_user_cell_data, returning_user_cell_data = [], [], []\n",
    "        \n",
    "        if update_interval.value == \"Daily\":\n",
    "            cell_data_date = start.strftime(\"%d %b, %Y\")     \n",
    "        if update_interval.value == \"Weekly\" or update_interval.value == \"No Separation\":\n",
    "            cell_data_date = start.strftime(\"%d %b, %Y\") + \" - \" + end.strftime(\"%d %b, %Y\")    \n",
    "        if update_interval.value == \"Monthly\":\n",
    "            cell_data_date = start.strftime(\"%b %Y\")\n",
    "        \n",
    "        response = response_rows = []\n",
    "        response = get_report(analytics, query)\n",
    "        print_response(response)\n",
    "        if \"rows\" in response[\"reports\"][0][\"data\"]:\n",
    "            response_rows = response[\"reports\"][0][\"data\"][\"rows\"]\n",
    "            \n",
    "        else:\n",
    "            response_rows = []\n",
    "            if feature.value == \"New/Returning Users\":\n",
    "                new_user_cell_data = [cell_data_date, 0]\n",
    "                new_user_data.append(new_user_cell_data)\n",
    "                returning_user_cell_data = [cell_data_date, 0]\n",
    "                returning_user_data.append(returning_user_cell_data)\n",
    "            else:\n",
    "                cell_data = [cell_data_date, 0]\n",
    "                data.append(cell_data)\n",
    "        \n",
    "        if feature.value == \"New/Returning Users\":\n",
    "            if response_rows != []:\n",
    "                new_user = False\n",
    "                returning_user = False\n",
    "                for res in response_rows:\n",
    "                    if (res[\"dimensions\"][0] == \"New Visitor\"):\n",
    "                        new_user_cell_data = [cell_data_date, int(res[\"metrics\"][0][\"values\"][0])]\n",
    "                        new_user_data.append(new_user_cell_data)\n",
    "                        new_user = True\n",
    "                    if (res[\"dimensions\"][0] == \"Returning Visitor\"):\n",
    "                        returning_user_cell_data = [cell_data_date, int(res[\"metrics\"][0][\"values\"][0])]\n",
    "                        returning_user_data.append(returning_user_cell_data)\n",
    "                        returning_user = True\n",
    "                if new_user == False:\n",
    "                    new_user_cell_data = [cell_data_date, 0]\n",
    "                    new_user_data.append(new_user_cell_data)\n",
    "                if returning_user == False:\n",
    "                    returning_user_cell_data = [cell_data_date, 0]\n",
    "                    returning_user_data.append(returning_user_cell_data)\n",
    "        else:\n",
    "            if response_rows != []:\n",
    "                response_present = False\n",
    "                for res in response_rows:\n",
    "                    if res[\"dimensions\"][0] == feature.value:\n",
    "                        cell_data = [cell_data_date, int(res[\"metrics\"][0][\"values\"][0])]\n",
    "                        data.append(cell_data)\n",
    "                        response_present = True\n",
    "                if response_present == False:\n",
    "                    cell_data = [cell_data_date, 0]\n",
    "                    data.append(cell_data)\n",
    "        \n",
    "        start, end = next_date_interval(start, end, update_interval.value)\n",
    "        \n",
    "folder_path = os.path.join(\"result_csv\", \"graph_data\")\n",
    "Path(folder_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df = new_df = returning_df = None\n",
    "if feature.value == \"New/Returning Users\":\n",
    "    \n",
    "    new_df = pd.DataFrame(new_user_data, columns = column_headers)\n",
    "    returning_df = pd.DataFrame(returning_user_data, columns = column_headers)\n",
    "    \n",
    "    new_action_column = new_df.iloc[:, 0]\n",
    "    new_frequency_column = new_df.iloc[:, 1]\n",
    "    new_x_markers = pd.Series(new_action_column).array\n",
    "    new_y_markers = pd.Series(new_frequency_column).array\n",
    "    new_y_markers = new_y_markers.astype(int)\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x = new_x_markers, y = new_y_markers, mode = 'lines', name = 'New Users'))\n",
    "    \n",
    "    ret_action_column = returning_df.iloc[:, 0]\n",
    "    ret_frequency_column = returning_df.iloc[:, 1]\n",
    "    ret_x_markers = pd.Series(ret_action_column).array\n",
    "    ret_y_markers = pd.Series(ret_frequency_column).array\n",
    "    ret_y_markers = ret_y_markers.astype(int)\n",
    "    \n",
    "    fig.add_trace(go.Scatter(x = ret_x_markers, y = ret_y_markers, mode = 'lines', name = 'Returning Users'))\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "else:\n",
    "    df = pd.DataFrame(data, columns = column_headers)\n",
    "    result_path = os.path.join(folder_path, file_name + \"_graph-\" + dt.strftime(\"%d %b, %Y\") + \" - \" + ds.strftime(\"%d %b, %Y\") + \".csv\")\n",
    "    df.to_csv(result_path, encoding='utf-8', index=False)    \n",
    "    df.astype({'Frequency': 'int32'}).dtypes\n",
    "    \n",
    "    fig = None\n",
    "\n",
    "    if update_interval.value == \"Daily\":\n",
    "        fig = px.line(df, x = \"Day\", y = \"Frequency\", render_mode = \"auto\", labels = {\"Day\": \"Date\",\"Frequency\": \"Frequency\"},\n",
    "            title = update_interval.value + \" Chart for '\" + feature.value + \"': \" + dt.strftime(\"%d %b, %Y\") + \" - \" + ds.strftime(\"%d %b, %Y\"))\n",
    "    if update_interval.value == \"Weekly\":\n",
    "        fig = px.line(df, x = \"Week\", y = \"Frequency\", render_mode = \"auto\", labels = {\"Day\": \"Week\",\"Frequency\": \"Frequency\"},\n",
    "            title = update_interval.value + \" Chart for '\" + feature.value + \"': \" + dt.strftime(\"%d %b, %Y\") + \" - \" + ds.strftime(\"%d %b, %Y\"))\n",
    "    if update_interval.value == \"Monthly\":\n",
    "        fig = px.line(df, x = \"Month\", y = \"Frequency\", render_mode = \"auto\", labels = {\"Day\": \"Month\",\"Frequency\": \"Frequency\"},\n",
    "            title = update_interval.value + \" Chart for '\" + feature.value + \"': \" + dt.strftime(\"%d %b, %Y\") + \" - \" + ds.strftime(\"%d %b, %Y\"))\n",
    "    if update_interval.value == \"No Separation\":\n",
    "        fig = px.scatter(df, x = \"Time Period\", y = \"Frequency\", render_mode = \"auto\", labels = {\"Day\": \"Time Period\",\"Frequency\": \"Frequency\"},\n",
    "            title = update_interval.value + \" Chart for '\" + feature.value + \"': \" + dt.strftime(\"%d %b, %Y\") + \" - \" + ds.strftime(\"%d %b, %Y\"))\n",
    "        fig.update_traces(marker={'size': 15})\n",
    "\n",
    "    fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946b2e93",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e40d4d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_7.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start = start_date.value\n",
    "end = end_date.value\n",
    "category_dict = {\n",
    "    \"Manage Dataset\": {},\n",
    "    \"App\": {},\n",
    "    \"Disseminate Dataset\": {},\n",
    "    \"Generate Dataset\": {},\n",
    "    \"Prepare Metadata\": {},\n",
    "    \"Other\": {}\n",
    "}\n",
    "\n",
    "ignore_list = [\"Establishing Python Connection\", \n",
    "               \"App Launched - OS\", \n",
    "               \"App Launched - SODA\",\n",
    "               \"App Restarted\",\n",
    "               \"Update Downloaded\",\n",
    "               \"Update Requested\",\n",
    "               \"Generate Dataset - Size\"\n",
    "              ]\n",
    "\n",
    "data = []\n",
    "\n",
    "query = {\n",
    "    'reportRequests': [\n",
    "    {\n",
    "        'viewId': VIEW_ID,\n",
    "        'dateRanges': [{'startDate': start.strftime('%Y-%m-%d'), 'endDate': end.strftime('%Y-%m-%d')}],\n",
    "        'metrics': [{'expression': 'ga:totalEvents'}],\n",
    "        'dimensions': [{'name': 'ga:eventAction'}]\n",
    "    }]\n",
    "}\n",
    "\n",
    "response = get_report(analytics, query)\n",
    "response_rows = response[\"reports\"][0][\"data\"][\"rows\"]\n",
    "\n",
    "for res in response_rows:\n",
    "    response_action = res[\"dimensions\"][0]\n",
    "    response_value = res[\"metrics\"][0][\"values\"][0]\n",
    "    \n",
    "    if(response_action in ignore_list):\n",
    "        continue\n",
    "    \n",
    "    action_found = False\n",
    "    for key in category_dict:\n",
    "        if (response_action.find(key) != -1 ):\n",
    "            if response_action in category_dict[key]:\n",
    "                category_dict[key][response_action] += response_value\n",
    "            else:\n",
    "                category_dict[key][response_action] = response_value\n",
    "            action_found = True\n",
    "            \n",
    "    if action_found == False:\n",
    "        if response_action.find(\"Manifest Files Created\") != -1:\n",
    "            if response_action in category_dict[\"Generate Dataset\"]:\n",
    "                category_dict[\"Generate Dataset\"][response_action] += response_value\n",
    "            else:\n",
    "                category_dict[\"Generate Dataset\"][response_action] = response_value\n",
    "        elif response_action in category_dict[\"Other\"]:\n",
    "            category_dict[\"Other\"][response_action] += response_value\n",
    "        else:\n",
    "            category_dict[\"Other\"][response_action] = response_value\n",
    "    \n",
    "for key in category_dict:\n",
    "    for action_key in category_dict[key]:\n",
    "        cell_data = [key, action_key, category_dict[key][action_key]]\n",
    "        data.append(cell_data)\n",
    "    \n",
    "df = pd.DataFrame(data, columns = [\"Action\", \"Subaction\", \"Total\"])\n",
    "result_path = os.path.join(\"test.csv\")\n",
    "df.to_csv(result_path, encoding='utf-8', index=False)\n",
    "\n",
    "fig = px.sunburst(df, path=['Action', 'Subaction'], values='Total',\n",
    "                  color='Subaction', hover_data=['Total'])\n",
    "fig.show()\n",
    "\n",
    "#fig.write_image(\"fig1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284e38cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad8df0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5933a7d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d25e8889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the dropdown values here\n",
    "dt = start_date.value\n",
    "ds = end_date.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "302f9d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date format in 'YYYY-MM-DD'\n",
    "# You can also use relative dates for simplicity\n",
    "# start_date = \"50daysAgo\"\n",
    "# end_date = \"today\"\n",
    "# end_date = \"yesterday\"\n",
    "# start_date = \"2021-01-23\"\n",
    "# end_date = \"2021-04-23\"\n",
    "\n",
    "# Comment this out to use the  regular format dates above \n",
    "start = dt.strftime('%Y-%m-%d')\n",
    "end = ds.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50d57c6-7b86-437a-9308-9793ee24a1ca",
   "metadata": {},
   "source": [
    "### Get a list of all datasets for which the actions below have been done on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c737a13e-7f82-4bd9-93b1-463dcebb88b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_actions = ['Manage Dataset - Create Empty Dataset', 'Manage Dataset - Change PI owner', 'Manage Dataset - Add User Permission',\n",
    "               'Manage Dataset - Add/Edit Subtitle', 'Manage Dataset - Add/Edit Description', 'Manage Dataset - Upload Banner Image', \n",
    "               'Manage Dataset - Assign License', 'Manage Dataset - Upload Local Dataset', 'Manage Dataset - Change Dataset Status',\n",
    "               'Prepare Metadata - Add Airtable account', 'Prepare Metadata - Add DDD', 'Prepare Metadata - Create Submission',\n",
    "               'Prepare Metadata - Create dataset_description', 'Generate Dataset - Local', 'Generate Dataset - Blackfynn',\n",
    "               'Generate Dataset - Pennsieve', 'Manifest Files Created - Blackfynn', 'Disseminate Dataset - Share with Curation Team', \n",
    "               'Disseminate Dataset - Share with Consortium', 'Disseminate Dataset - Pre-publishing Review', 'Download Template - submission.xlsx', \n",
    "               'Download Template - subjects.xlsx', 'Download Template - samples.xlsx', 'Download Template - dataset_description.xlsx',\n",
    "               'Upload Local Dataset - Number of Folders', 'Upload Local Dataset - Number of Files', 'Prepare Metadata - Add Airtable account',\n",
    "               'Manifest Files Created - Local', 'Generate Dataset - Local - Size', 'Prepare Metadata - Add DDD', 'Manifest Files Created -', 'Generate Dataset -', 'Manifest Files Created',\n",
    "               'Generate Dataset - Size', 'Generate Dataset - Number of Files', 'Generate Dataset', \n",
    "              ]\n",
    "\n",
    "# start_date = \"2021-05-01\"\n",
    "# end_date = \"2021-05-30\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eea237bb-f98d-4cd3-9506-b464ef4a3f71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "212d854240184444bc4c52fc1705adc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 ||||| ['!#$%5', '2021-02-19', '2021-02-19-lab', 'Confocal images of Yucatan pig right atrial ganglionated plexus (RAGP) whole-mounts', 'Effects of Temporal Patterns of Vagus Nerve Stimulation on Heart Rate in Mouse', 'Fabbri et al. (2017) model', 'fsfas', 'Functional mapping with lumbosacral epidural stimulation for restoration of bladder function after spinal cord injury in rats (T13)', 'Lightsheet', 'new_dataset_tram', 'Pudendal Nerve mapping', 'Right atrial ganglionated plexus ablation', 'scRNA-Seq Reveals New Enteric Nervous System Roles for GDNF, NRTN, and TBX3', 'Simulation of rat pelvic and vagus fascicle electrical stimulation and recording', 'Simulations of Pelvic and Vagus neural interface anatomy-dependent stimulus and recording properties', 'SODA 4.20(2) pennisieve beat test', 'SODA_Task1_MKBains', 'SODA4.20 pennisieve beta test', 'Sun Lab Human Dataset 2', 'Sun Lab Human Lung Dataset 1', 'Task1_MKBains_version3', 'Temporal Patterns of Stimulation of the Vagus Nerve Differentially Affect Heart Rate in Mice', 'Temporal Patterns of Stimulation of the Vagus Nerve Differentially Affects Heart Rate in Mice', 'test-new-dataset12', 'test-ps-SODA', 'Vagal MEA recording for Gastroparetic Patients', 'Ablation of the intrinsic cardiac nervous system to evaluate efferent control of cardiac function', 'Pudendal Nerve Mapping', 'SODA 4.20(2) pennisieve beta test', 'test-ps-SODA-renamed', 'SODA-dataset-test', 'Functional neuronal nodose recording from pig - Cardiac field chemical and mechanical stimulation', 'ViNERS (Visceral Nerve Ensemble Recording & Stimulation) peripheral neural interface modelling environment', 'Endoscopic access of vagus nerve', 'Endoscopic access of Vagus nerve', 'b-workstation', 'iMAC Pennisieve Key', 'SODA-Pennsieve', 'tram-demo', 'new_dataset_tram_renamed_2', 'Select dataset', 'SODA-beta-test-update-renamed', 'Cardiac GP Ablation', 'dataset-sparc-keast-bi-001', 'Effect of Vagus Nerve Stimulation Parameter Selection on Heart Rate in Anesthetized Mice', 'Identification of lung innervating sensory neurons and their target specificity', 'iDisco', 'Monosynaptic circuit mapping of interscapular brown adipose tissues (iBAT) in mice', 'SODA OUTPUT', 'SODA-001', 'soda7', 'SODAA', '4D Upper Gastrointestinal Magnetic Resonance Imaging in Healthy Human Subjects and Gastroparetic Patients', 'A spatially-tracked single cell transcriptomics map of neuronal networks in the intrinsic cardiac nervous system', 'Anka testing dataset', 'Functional Magnetic Resonance Imaging Under Swallowing Task in Healthy Human Subjects and Gastroparetic Patients', 'new_0603', 'Quantitation of nodose neurons labelled by retrograde tracing from selective gastric mucosal and gastric muscle injection in the antrum and fundus', 'Quantitation of nodose neurons labelled by retrograde tracing from selective injections into the gastric mucosa or muscle in the antrum and fundus', 'Resting-state Functional Magnetic Resonance Imaging Under Fast and Fed States in Healthy Human Subjects and Gastroparetic Patients', 'RNAseq analysis of nodose neurons projecting to specific regions and tissue layers within the rat stomach', 'Sun Lab Single Nucleus RNAseq Dataset', 'backup-data-3', '(not set)']\n"
     ]
    }
   ],
   "source": [
    "dataset_list = []\n",
    "def datasets_and_actions(start, end, category, action):\n",
    "    query = {\n",
    "        'reportRequests': [\n",
    "        {\n",
    "            'viewId': VIEW_ID,\n",
    "            'dateRanges': [{'startDate': start, 'endDate': end}],\n",
    "            'metrics': [{'expression': 'ga:totalEvents'}],\n",
    "            'dimensions': [{'name': 'ga:eventCategory'}, {'name': 'ga:eventAction'}, {'name': 'ga:eventLabel'}]\n",
    "        }]\n",
    "    }\n",
    "    response = get_report(analytics, query)\n",
    "    response_rows = response[\"reports\"][0][\"data\"][\"rows\"]\n",
    "    data = []\n",
    "\n",
    "    for res in response_rows:\n",
    "        if res[\"dimensions\"][0] == category:\n",
    "            if res[\"dimensions\"][1] == action:\n",
    "                cell_data = [res[\"dimensions\"][2], res[\"metrics\"][0][\"values\"][0]]\n",
    "                val = res[\"dimensions\"][2]\n",
    "                if val not in dataset_list:\n",
    "                    dataset_list.append(val)\n",
    "#                 data.append(cell_data)\n",
    "\n",
    "        \n",
    "    folder_path = os.path.join(\"result_csv\", \"custom\")\n",
    "    Path(folder_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#     df = pd.DataFrame(data, columns = ['Dataset_name', 'Values']\n",
    "#     result_path = os.path.join(folder_path, action + \"-\" + start_date + \"_\" + end_date + \".csv\")\n",
    "#     df.to_csv(result_path, encoding='utf-8', index=False)\n",
    "    return\n",
    "\n",
    "for i in trange(len(all_actions)):\n",
    "    action = all_actions[i]\n",
    "    datasets_and_actions(start, end, \"Success\", action)    \n",
    "\n",
    "filtered_dataset_list = list(filter(lambda item: len(item) > 4, dataset_list))\n",
    "print(len(filtered_dataset_list), \"|||||\", filtered_dataset_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd927c1a",
   "metadata": {},
   "source": [
    "### Get a report of all the events that occurred with a status of either \"success\" or \"error\" within a given time frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2b44aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {\n",
    "    'reportRequests': [\n",
    "    {\n",
    "        'viewId': VIEW_ID,\n",
    "        'dateRanges': [{'startDate': start, 'endDate': end}],\n",
    "        'metrics': [{'expression': 'ga:totalEvents'}],\n",
    "        'dimensions': [{'name': 'ga:eventCategory'}, {'name': 'ga:eventAction'}]\n",
    "    }]\n",
    "}\n",
    "\n",
    "response = get_report(analytics, query)\n",
    "response_rows = response[\"reports\"][0][\"data\"][\"rows\"]\n",
    "both_data, success_data, error_data = [], [], []\n",
    "\n",
    "for res in response_rows:\n",
    "    both_cell_data = [res[\"dimensions\"][0], res[\"dimensions\"][1], res[\"metrics\"][0][\"values\"][0]]\n",
    "    both_data.append(both_cell_data)\n",
    "    if res[\"dimensions\"][0] == \"Success\":\n",
    "        success_cell_data = [res[\"dimensions\"][1], res[\"metrics\"][0][\"values\"][0]]\n",
    "        success_data.append(success_cell_data)\n",
    "    if res[\"dimensions\"][0] == \"Error\":\n",
    "        error_cell_data = [res[\"dimensions\"][1], res[\"metrics\"][0][\"values\"][0]]\n",
    "        error_data.append(error_cell_data)\n",
    "\n",
    "folder_path = os.path.join(\"result_csv\", \"status_count\")\n",
    "Path(folder_path).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "df = pd.DataFrame(both_data, columns = ['Status', 'Action', 'Values'])\n",
    "result_path = os.path.join(folder_path, \"Both-\" + start + \"_\" + end + \".csv\")\n",
    "df.to_csv(result_path, encoding='utf-8', index=False)\n",
    "\n",
    "df = pd.DataFrame(success_data, columns = ['Action', 'Values'])\n",
    "result_path = os.path.join(folder_path, \"Success-\" + start + \"_\" + end + \".csv\")\n",
    "df.to_csv(result_path, encoding='utf-8', index=False)\n",
    "\n",
    "df = pd.DataFrame(error_data, columns = ['Action', 'Values'])\n",
    "result_path = os.path.join(folder_path, \"Error-\" + start + \"_\" + end + \".csv\")\n",
    "df.to_csv(result_path, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c376beba",
   "metadata": {},
   "source": [
    "### Get a report of all app launches within a given time frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6c9f7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {\n",
    "    'reportRequests': [\n",
    "    {\n",
    "        'viewId': VIEW_ID,\n",
    "        'dateRanges': [{'startDate': start, 'endDate': end}],\n",
    "        'metrics': [{'expression': 'ga:totalEvents'}],\n",
    "        'dimensions': [{'name': 'ga:eventAction'}, {'name': 'ga:eventLabel'}]\n",
    "    }]\n",
    "}\n",
    "\n",
    "response = get_report(analytics, query)\n",
    "response_rows = response[\"reports\"][0][\"data\"][\"rows\"]\n",
    "app_launch_os, app_launch_soda = [], []\n",
    "\n",
    "for res in response_rows:\n",
    "    if res[\"dimensions\"][0] == \"App Launched - OS\":\n",
    "        app_launch_os_cell_data = [res[\"dimensions\"][1], res[\"metrics\"][0][\"values\"][0]]\n",
    "        app_launch_os.append(app_launch_os_cell_data)\n",
    "    if res[\"dimensions\"][0] == \"App Launched - SODA\":\n",
    "        app_launch_soda_cell_data = [res[\"dimensions\"][1], res[\"metrics\"][0][\"values\"][0]]\n",
    "        app_launch_soda.append(app_launch_soda_cell_data)\n",
    "\n",
    "folder_path = os.path.join(\"result_csv\", \"app_launched\")\n",
    "Path(folder_path).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "df = pd.DataFrame(app_launch_os, columns = ['OS', 'Values'])\n",
    "result_path = os.path.join(folder_path, \"os-\" + start + \"_\" + end + \".csv\")\n",
    "df.to_csv(result_path, encoding='utf-8', index=False)\n",
    "\n",
    "df = pd.DataFrame(app_launch_soda, columns = ['SODA Version', 'Values'])\n",
    "result_path = os.path.join(folder_path, \"soda_version-\" + start + \"_\" + end + \".csv\")\n",
    "df.to_csv(result_path, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5916f8",
   "metadata": {},
   "source": [
    "### Get a report of all unique users within a given time frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "889860bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {\n",
    "    'reportRequests': [\n",
    "    {\n",
    "        'viewId': VIEW_ID,\n",
    "        'dateRanges': [{'startDate': start, 'endDate': end}],\n",
    "        'metrics': [{'expression': 'ga:users'}],\n",
    "        'dimensions': [{'name': 'ga:userType'}]\n",
    "    }]\n",
    "}\n",
    "\n",
    "response = get_report(analytics, query)\n",
    "response_rows = response[\"reports\"][0][\"data\"][\"rows\"]\n",
    "data = []\n",
    "\n",
    "for res in response_rows:\n",
    "    cell_data = [res[\"dimensions\"][0], res[\"metrics\"][0][\"values\"][0]]\n",
    "    data.append(cell_data)\n",
    "    \n",
    "folder_path = os.path.join(\"result_csv\", \"users\")\n",
    "Path(folder_path).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "df = pd.DataFrame(data, columns = ['Type', 'Values'])\n",
    "result_path = os.path.join(folder_path, \"users-\" + start + \"_\" + end + \".csv\")\n",
    "df.to_csv(result_path, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0fd46f",
   "metadata": {},
   "source": [
    "### Get a report of all dataset names for a specific action for a given time frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37ceae09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_actions(start, end, category, action):\n",
    "    query = {\n",
    "        'reportRequests': [\n",
    "        {\n",
    "            'viewId': VIEW_ID,\n",
    "            'dateRanges': [{'startDate': start, 'endDate': end}],\n",
    "            'metrics': [{'expression': 'ga:totalEvents'}],\n",
    "            'dimensions': [{'name': 'ga:eventCategory'}, {'name': 'ga:eventAction'}, {'name': 'ga:eventLabel'}]\n",
    "        }]\n",
    "    }\n",
    "    response = get_report(analytics, query)\n",
    "    response_rows = response[\"reports\"][0][\"data\"][\"rows\"]\n",
    "    data = []\n",
    "\n",
    "    for res in response_rows:\n",
    "        if res[\"dimensions\"][0] == category:\n",
    "            if res[\"dimensions\"][1] == action:\n",
    "                cell_data = [res[\"dimensions\"][2], res[\"metrics\"][0][\"values\"][0]]\n",
    "                data.append(cell_data)\n",
    "        \n",
    "    folder_path = os.path.join(\"result_csv\", \"custom\")\n",
    "    Path(folder_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    df = pd.DataFrame(data, columns = ['Dataset_name', 'Values'])\n",
    "    result_path = os.path.join(folder_path, action + \"-\" + start + \"_\" + end + \".csv\")\n",
    "    df.to_csv(result_path, encoding='utf-8', index=False)\n",
    "    return\n",
    "\n",
    "## useful for getting the names of datasets where an action is applicable\n",
    "## all responses go to the custom folder\n",
    "# number_of_actions(start_date, end_date, <type>, <action_name>)\n",
    "number_of_actions(start, end, \"Success\", \"Manage Dataset - Create Empty Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888106ff",
   "metadata": {},
   "source": [
    "### Get a report of all actions done on a specific dataset for a given time frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3949acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_actions(start, end, dataset_name):\n",
    "    query = {\n",
    "        'reportRequests': [\n",
    "        {\n",
    "            'viewId': VIEW_ID,\n",
    "            'dateRanges': [{'startDate': start, 'endDate': end}],\n",
    "            'metrics': [{'expression': 'ga:totalEvents'}],\n",
    "            'dimensions': [{'name': 'ga:eventCategory'}, {'name': 'ga:eventAction'}, {'name': 'ga:eventLabel'}]\n",
    "        }]\n",
    "    }\n",
    "    response = get_report(analytics, query)\n",
    "    response_rows = response[\"reports\"][0][\"data\"][\"rows\"]\n",
    "    data = []\n",
    "\n",
    "    for res in response_rows:\n",
    "        if res[\"dimensions\"][2].find(dataset_name) != -1 or res[\"dimensions\"][1].find(dataset_name) != -1:\n",
    "            cell_data = [res[\"dimensions\"][0], res[\"dimensions\"][1], res[\"metrics\"][0][\"values\"][0]]\n",
    "            data.append(cell_data)\n",
    "        \n",
    "    folder_path = os.path.join(\"result_csv\", \"custom\")\n",
    "    Path(folder_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    df = pd.DataFrame(data, columns = ['Status', 'Action', 'Values'])\n",
    "    result_path = os.path.join(folder_path, dataset_name + \"(actions)-\" + start + \"_\" + end + \".csv\")\n",
    "    df.to_csv(result_path, encoding='utf-8', index=False)\n",
    "    return\n",
    "\n",
    "## useful for getting all the actions for a specific dataset\n",
    "## all responses go to the custom folder\n",
    "# dataset_actions(start_date, end_date, <Dataset_name>)\n",
    "dataset_actions(start, end, \"test-ps-SODA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c5982f",
   "metadata": {},
   "source": [
    "### Get the number of files and the size of all datasets that was uploaded through SODA for a given time frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b1253922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_statistics(start, end):\n",
    "            \n",
    "    query = {\n",
    "        'reportRequests': [\n",
    "        {\n",
    "            'viewId': VIEW_ID,\n",
    "            'dateRanges': [{'startDate': start, 'endDate': end}],\n",
    "            'metrics': [{'expression': 'ga:uniqueEvents'}],\n",
    "            'dimensions': [{'name': 'ga:eventCategory'}, {'name': 'ga:eventAction'}, {'name': 'ga:eventLabel'}]\n",
    "        }]\n",
    "    }\n",
    "    response = get_report(analytics, query)\n",
    "    response_rows = response[\"reports\"][0][\"data\"][\"rows\"]\n",
    "    \n",
    "    for res in response_rows:\n",
    "#         print(res)\n",
    "        if res[\"dimensions\"][1] == \"Upload Local Dataset - Number of Files\":\n",
    "            cell_data = [res[\"dimensions\"][0], res[\"dimensions\"][2], 0]\n",
    "            data.append(cell_data)\n",
    "        if res[\"dimensions\"][1] == \"Upload Local Dataset - size\":\n",
    "            cell_data = [res[\"dimensions\"][0], 0, res[\"dimensions\"][2]]\n",
    "            data.append(cell_data)\n",
    "        if res[\"dimensions\"][1] == \"Generate Dataset - Number of Files\":\n",
    "            cell_data = [res[\"dimensions\"][0], res[\"dimensions\"][2], 0]\n",
    "            data.append(cell_data)\n",
    "        if res[\"dimensions\"][1] == \"Generate Dataset - Size\":\n",
    "            cell_data = [res[\"dimensions\"][0], 0, res[\"dimensions\"][2]]\n",
    "            data.append(cell_data)\n",
    "        \n",
    "    folder_path = os.path.join(\"result_csv\", \"custom\")\n",
    "    Path(folder_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    df = pd.DataFrame(data, columns = ['Status', 'Number of Files', 'Size in (bytes)'])\n",
    "    result_path = os.path.join(folder_path, \"dataset_statistics-\" + start + \"_\" + end + \".csv\")\n",
    "    df.to_csv(result_path, encoding='utf-8', index=False)\n",
    "    return\n",
    "\n",
    "## useful for getting all details for upload to Pennsieve for a specific time period\n",
    "## all responses go to the custom folder\n",
    "# num_of_files_folders_in_dataset(start_date, end_date)\n",
    "dataset_statistics(start, end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab95213",
   "metadata": {},
   "source": [
    "### Get the number of files and the size of a specific dataset that was uploaded through SODA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7f6306b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_of_files_folders_in_dataset(start_date, end, dataset_name):\n",
    "    query = {\n",
    "        'reportRequests': [\n",
    "        {\n",
    "            'viewId': VIEW_ID,\n",
    "            'dateRanges': [{'startDate': start, 'endDate': end}],\n",
    "            'metrics': [{'expression': 'ga:totalEvents'}],\n",
    "            'dimensions': [{'name': 'ga:eventCategory'}, {'name': 'ga:eventAction'}, {'name': 'ga:eventLabel'}]\n",
    "        }]\n",
    "    }\n",
    "    response = get_report(analytics, query)\n",
    "    response_rows = response[\"reports\"][0][\"data\"][\"rows\"]\n",
    "    data = []\n",
    "    \n",
    "    for res in response_rows:\n",
    "        if res[\"dimensions\"][1].find(\"Upload Local Dataset\") != -1:\n",
    "            if res[\"dimensions\"][1].find(dataset_name) != -1:\n",
    "                if res[\"dimensions\"][1].find('Number of Files') != -1:\n",
    "                    cell_data = [res[\"dimensions\"][0], res[\"dimensions\"][2], 0]\n",
    "                    data.append(cell_data)\n",
    "                if res[\"dimensions\"][1].find('- size') != -1:\n",
    "                    cell_data = [res[\"dimensions\"][0], 0, res[\"dimensions\"][2]]\n",
    "                    data.append(cell_data)\n",
    "            \n",
    "    query = {\n",
    "        'reportRequests': [\n",
    "        {\n",
    "            'viewId': VIEW_ID,\n",
    "            'dateRanges': [{'startDate': start, 'endDate': end}],\n",
    "            'metrics': [{'expression': 'ga:eventValue'}],\n",
    "            'dimensions': [{'name': 'ga:eventCategory'}, {'name': 'ga:eventAction'}, {'name': 'ga:eventLabel'}]\n",
    "        }]\n",
    "    }\n",
    "    response = get_report(analytics, query)\n",
    "    response_rows = response[\"reports\"][0][\"data\"][\"rows\"]\n",
    "    res[\"metrics\"][0][\"values\"][0]\n",
    "    for res in response_rows:\n",
    "        if res[\"dimensions\"][1].find(\"Generate Dataset\") != -1:\n",
    "            if res[\"dimensions\"][2].find(dataset_name) != -1:\n",
    "                if res[\"dimensions\"][1] == 'Generate Dataset - Number of Files':\n",
    "                    cell_data = [res[\"dimensions\"][0], res[\"metrics\"][0][\"values\"][0], 0]\n",
    "                    data.append(cell_data)\n",
    "                if res[\"dimensions\"][1] == 'Generate Dataset - Size':\n",
    "                    cell_data = [res[\"dimensions\"][0], 0, res[\"metrics\"][0][\"values\"][0]]\n",
    "                    data.append(cell_data)\n",
    "                \n",
    "                    \n",
    "    folder_path = os.path.join(\"result_csv\", \"custom\")\n",
    "    Path(folder_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    df = pd.DataFrame(data, columns = ['Status', 'Number of Files', 'Size in (bytes)'])\n",
    "    result_path = os.path.join(folder_path, dataset_name + \"(details)-\" + start + \"_\" + end + \".csv\")\n",
    "    df.to_csv(result_path, encoding='utf-8', index=False)\n",
    "    return\n",
    "\n",
    "## useful for getting all the number of files and size for a specific dataset\n",
    "## all responses go to the custom folder\n",
    "# num_of_files_folders_in_dataset(start_date, end_date, <Dataset_name>)\n",
    "num_of_files_folders_in_dataset(start, end, \"test-ps-SODA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458b2d1a",
   "metadata": {},
   "source": [
    "### Get the number of files and the size of all datasets that was uploaded through SODA for a given time frameGet the number of files and the size of all datasets that was uploaded through SODA for a given time frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "21bd45c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dimensions': ['Success', 'Manage Datasets - Upload local dataset - Number of Files', 'Number of files local dataset'], 'metrics': [{'values': ['1', '11']}]}\n",
      "{'dimensions': ['Success', 'Manage Datasets - Upload local dataset - size', 'Upload local dataset'], 'metrics': [{'values': ['1', '58592']}]}\n",
      "{'dimensions': ['Success', 'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Number of Files', 'N:dataset:a57263bb-132b-4af0-bd88-140ad9592efc'], 'metrics': [{'values': ['1', '3']}]}\n",
      "{'dimensions': ['Success', 'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Number of Files', 'New'], 'metrics': [{'values': ['1', '3']}]}\n",
      "{'dimensions': ['Success', 'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Size', 'N:dataset:a57263bb-132b-4af0-bd88-140ad9592efc'], 'metrics': [{'values': ['1', '5629']}]}\n",
      "{'dimensions': ['Success', 'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Size', 'New'], 'metrics': [{'values': ['1', '0']}]}\n",
      "{'dimensions': ['Success', 'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Size', 'Size'], 'metrics': [{'values': ['1', '5629']}]}\n"
     ]
    }
   ],
   "source": [
    "def dataset_statistics_update(start, end):\n",
    "            \n",
    "    query = {\n",
    "        'reportRequests': [\n",
    "        {\n",
    "            'viewId': VIEW_ID,\n",
    "            'dateRanges': [{'startDate': start, 'endDate': end}],\n",
    "            'metrics': [{'expression': 'ga:uniqueEvents'}, {'expression': 'ga:eventValue'}],\n",
    "            'dimensions': [{'name': 'ga:eventCategory'}, {'name': 'ga:eventAction'}, \n",
    "                           {'name': 'ga:eventLabel'}],\n",
    "\n",
    "        }]\n",
    "    }\n",
    "    response = get_report(analytics, query)\n",
    "    response_rows = response[\"reports\"][0][\"data\"][\"rows\"]\n",
    "    \n",
    "    for res in response_rows:\n",
    "        \n",
    "        if res[\"dimensions\"][1] == \"Manage Datasets - Upload local dataset - Number of Files\":\n",
    "            print(res)\n",
    "            cell_data = [res[\"dimensions\"][0], res[\"dimensions\"][2], 0]\n",
    "            data.append(cell_data)\n",
    "        if res[\"dimensions\"][1] == \"Manage Datasets - Upload local dataset - size\":\n",
    "            print(res)\n",
    "            cell_data = [res[\"dimensions\"][0], 0, res[\"dimensions\"][2]]\n",
    "            data.append(cell_data)\n",
    "        if res[\"dimensions\"][1] == \"Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Number of Files\":\n",
    "            print(res)\n",
    "            cell_data = [res[\"dimensions\"][0], res[\"dimensions\"][2], 0]\n",
    "            data.append(cell_data)\n",
    "        if res[\"dimensions\"][1] == \"Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Size\":\n",
    "            print(res)\n",
    "            cell_data = [res[\"dimensions\"][0], 0, res[\"dimensions\"][2]]\n",
    "            data.append(cell_data)\n",
    "        \n",
    "    folder_path = os.path.join(\"result_csv\", \"custom\")\n",
    "    Path(folder_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    df = pd.DataFrame(data, columns = ['Status', 'Number of Files', 'Size in (bytes)'])\n",
    "    result_path = os.path.join(folder_path, \"dataset_statistics-\" + start + \"_\" + end + \".csv\")\n",
    "    df.to_csv(result_path, encoding='utf-8', index=False)\n",
    "    return\n",
    "\n",
    "## useful for getting all details for upload to Pennsieve for a specific time period\n",
    "## all responses go to the custom folder\n",
    "# num_of_files_folders_in_dataset(start_date, end_date)\n",
    "dataset_statistics_update(start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66be4baf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
