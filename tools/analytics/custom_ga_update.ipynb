{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SODA for SPARC reports 2022 Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: oauth2client in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (4.1.3)\n",
      "Requirement already satisfied: httplib2>=0.9.1 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from oauth2client) (0.20.4)\n",
      "Requirement already satisfied: six>=1.6.1 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from oauth2client) (1.16.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from oauth2client) (0.2.8)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from oauth2client) (0.4.8)\n",
      "Requirement already satisfied: rsa>=3.1.4 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from oauth2client) (4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from httplib2>=0.9.1->oauth2client) (3.0.7)\n",
      "Requirement already satisfied: google-api-python-client in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (2.38.0)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.15.0 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from google-api-python-client) (0.20.4)\n",
      "Requirement already satisfied: google-api-core<3.0.0dev,>=1.21.0 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from google-api-python-client) (2.5.0)\n",
      "Requirement already satisfied: google-auth<3.0.0dev,>=1.16.0 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from google-api-python-client) (2.6.0)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.1.0 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from google-api-python-client) (0.1.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from google-api-python-client) (4.1.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.52.0 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from google-api-core<3.0.0dev,>=1.21.0->google-api-python-client) (1.55.0)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from google-api-core<3.0.0dev,>=1.21.0->google-api-python-client) (3.19.4)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from google-api-core<3.0.0dev,>=1.21.0->google-api-python-client) (2.27.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from google-auth<3.0.0dev,>=1.16.0->google-api-python-client) (4.0)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from google-auth<3.0.0dev,>=1.16.0->google-api-python-client) (1.16.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from google-auth<3.0.0dev,>=1.16.0->google-api-python-client) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from google-auth<3.0.0dev,>=1.16.0->google-api-python-client) (4.2.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from httplib2<1dev,>=0.15.0->google-api-python-client) (3.0.7)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=1.16.0->google-api-python-client) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3.0.0dev,>=1.21.0->google-api-python-client) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3.0.0dev,>=1.21.0->google-api-python-client) (1.26.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3.0.0dev,>=1.21.0->google-api-python-client) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3.0.0dev,>=1.21.0->google-api-python-client) (2.0.12)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (7.6.5)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from ipywidgets) (5.1.3)\n",
      "Requirement already satisfied: ipython>=4.0.0 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from ipywidgets) (7.16.3)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from ipywidgets) (1.0.2)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from ipywidgets) (4.3.3)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from ipywidgets) (5.5.6)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from ipywidgets) (3.5.2)\n",
      "Requirement already satisfied: tornado>=4.2 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (6.1)\n",
      "Requirement already satisfied: jupyter-client in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (7.1.2)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (3.0.28)\n",
      "Requirement already satisfied: colorama in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (0.4.4)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (58.0.4)\n",
      "Requirement already satisfied: decorator in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: jedi<=0.17.2,>=0.10 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (0.17.2)\n",
      "Requirement already satisfied: pygments in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (2.11.2)\n",
      "Requirement already satisfied: backcall in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from jedi<=0.17.2,>=0.10->ipython>=4.0.0->ipywidgets) (0.7.1)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from nbformat>=4.2.0->ipywidgets) (3.2.0)\n",
      "Requirement already satisfied: jupyter-core in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from nbformat>=4.2.0->ipywidgets) (4.9.2)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (21.4.0)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (1.16.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (0.18.0)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (4.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: notebook>=4.4.1 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from widgetsnbextension~=3.5.0->ipywidgets) (6.4.8)\n",
      "Requirement already satisfied: pyzmq>=17 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (22.3.0)\n",
      "Requirement already satisfied: argon2-cffi in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (21.3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (3.0.3)\n",
      "Requirement already satisfied: nbconvert in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (6.0.7)\n",
      "Requirement already satisfied: prometheus-client in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.13.1)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.8.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.12.1)\n",
      "Requirement already satisfied: nest-asyncio>=1.5 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.5.4)\n",
      "Requirement already satisfied: entrypoints in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
      "Requirement already satisfied: pywin32>=1.0 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from jupyter-core->nbformat>=4.2.0->ipywidgets) (223)\n",
      "Requirement already satisfied: pywinpty>=1.1.0 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from terminado>=0.8.3->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.1.6)\n",
      "Requirement already satisfied: argon2-cffi-bindings in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (21.2.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (4.1.1)\n",
      "Requirement already satisfied: dataclasses in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.8)\n",
      "Requirement already satisfied: cffi>=1.0.1 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.15.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.21)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from importlib-metadata->jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (3.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: testpath in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.6.0)\n",
      "Requirement already satisfied: bleach in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (4.1.0)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.1.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.5.0)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.9)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.7.1)\n",
      "Requirement already satisfied: async-generator in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.10)\n",
      "Requirement already satisfied: packaging in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (21.3)\n",
      "Requirement already satisfied: webencodings in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from packaging->bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (3.0.7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: plotly in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (5.6.0)\n",
      "Requirement already satisfied: six in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from plotly) (1.16.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from plotly) (8.0.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (4.63.1)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from tqdm) (5.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from tqdm) (0.4.4)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\cmarroquin\\anaconda3\\envs\\env-electron-python\\lib\\site-packages (from importlib-resources->tqdm) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install oauth2client\n",
    "!pip install google-api-python-client\n",
    "!pip install ipywidgets\n",
    "!pip install plotly\n",
    "!pip install tqdm -U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-requisite Installations if Needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import ipywidgets as widgets\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import calendar\n",
    "\n",
    "import uuid\n",
    "\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "\n",
    "from apiclient.discovery import build\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "\n",
    "import httplib2\n",
    "from oauth2client import client\n",
    "from oauth2client import file\n",
    "from oauth2client import tools\n",
    "from helper_functions import initialize_analyticsreporting, get_report, print_response, VIEW_ID, next_date_interval, progress_bar_counter\n",
    "\n",
    "pio.renderers.default = \"iframe\"\n",
    "\n",
    "analytics = initialize_analyticsreporting()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The cell below will render the widgets needed to select the items in the graph. This cell only needs to be run ONCE (to show the widgets only). After display, you don't have to run this cell. The report/graph will include the end date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0173c745e074522b9c778fdccb23cbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Option:', options=('New/Returning Users', 'App Launched - OS', 'App Launched - SODA', 'M…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5533c4526fb7412b8ddebed43797c2e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "DatePicker(value=None, description='Start Date:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fff7de1a53f0415c953f01ca7ea641dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "DatePicker(value=None, description='End Date:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "062847b609c34bd1afd7bf37e2a57479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Update Interval:', options=('Daily', 'Weekly', 'Monthly', 'No Separation'), value='Daily…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature = widgets.Dropdown(\n",
    "    options=[\n",
    "        'New/Returning Users',\n",
    "        'App Launched - OS', \n",
    "        'App Launched - SODA', \n",
    "        'Manage Datasets - Create a new dataset',\n",
    "        'Manage Datasets - Rename an existing dataset',\n",
    "        'Manage Datasets - Make PI owner of dataset',\n",
    "        'Manage Datasets - Add/Edit Permissions',\n",
    "        'Manage Datasets - Add/Edit Permissions - Add User Permissions',\n",
    "        'Manage Datasets - Add/Edit Permissions - Add Team Permissions',\n",
    "        \"Manage Datasets - Add/Edit Subtitle\",\n",
    "        \"Manage Datasets - Add/Edit Subtitle - Get Subtitle\",\n",
    "        \"Manage Datasets - Add/Edit Readme\",\n",
    "        \"Manage Datasets - Add/Edit Readme - Get Readme\",\n",
    "        \"Manage Datasets - Add/Edit Readme - Parse Readme\",  \n",
    "        \"Manage Datasets - Upload a Banner Image\",\n",
    "        \"Manage Datasets - Upload a Banner Image - Size\",\n",
    "        \"Manage Datasets - Upload a Banner Image - Importing Banner Image\",\n",
    "        \"Manage Datasets - Upload a Banner Image - Get Banner Image\",\n",
    "        \"Manage Datasets - Add/Edit Tags\",\n",
    "        \"Manage Datasets - Add/Edit Tags - Get Tags\",\n",
    "        \"Manage Datasets - Assign a License\",\n",
    "        \"Manage Datasets - Assign a License - Get License\",\n",
    "        \"Manage Datasets - Upload Local Dataset\",\n",
    "        \"Manage Datasets - Upload Local Dataset - size\",\n",
    "        \"Manage Datasets - Upload Local Dataset - name - size\",\n",
    "        \"Manage Datasets - Upload Local Dataset - Number of Folders\",\n",
    "        \"Manage Datasets - Upload Local Dataset - name - Number of Folders\",\n",
    "        \"Manage Datasets - Upload Local Dataset - Number of Files\",\n",
    "        \"Manage Datasets - Upload Local Dataset - name - Number of Files\",\n",
    "        \"Manage Datasets - Change Dataset Status\",\n",
    "        \"Manage Datasets - Change Dataset Status - Get Dataset Status\"\n",
    "        \n",
    "        \n",
    "        \n",
    "        'Prepare Metadata - Add Airtable account',\n",
    "        'Prepare Metadata - Add DDD',\n",
    "        'Prepare Metadata - Create Submission',\n",
    "        'Prepare Metadata - Create dataset_description',\n",
    "        'Prepare Metadata - samples',\n",
    "        'Prepare Metadata - samples - Generate',\n",
    "        'Prepare Metadata - samples - Generate - Local',\n",
    "        'Prepare Metadata - samples - Generate - Pennsieve',\n",
    "        'Prepare Metadata - samples - Existing',\n",
    "        'Prepare Metadata - samples - Existing - Local',\n",
    "        'Prepare Metadata - samples - Existing - Pennsieve',\n",
    "        'Prepare Metadata - submission',\n",
    "        'Prepare Metadata - submission - Generate',\n",
    "        'Prepare Metadata - submission - Generate - Local',\n",
    "        'Prepare Metadata - submission - Generate - Pennsieve',\n",
    "        'Prepare Metadata - submission - Existing',\n",
    "        'Prepare Metadata - submission - Existing - Local',\n",
    "        'Prepare Metadata - submission - Existing - Pennsieve',\n",
    "        'Prepare Metadata - dataset_description',\n",
    "        'Prepare Metadata - dataset_description - Generate',\n",
    "        'Prepare Metadata - dataset_description - Generate - Local',\n",
    "        'Prepare Metadata - dataset_description - Generate - Pennsieve',\n",
    "        'Prepare Metadata - dataset_description - Existing',\n",
    "        'Prepare Metadata - dataset_description - Existing - Local',\n",
    "        'Prepare Metadata - dataset_description - Existing - Pennsieve',\n",
    "        'Prepare Metadata - subjects',\n",
    "        'Prepare Metadata - subjects - Generate',\n",
    "        'Prepare Metadata - subjects - Generate - Local',\n",
    "        'Prepare Metadata - subjects - Generate - Pennsieve',\n",
    "        'Prepare Metadata - subjects - Existing',\n",
    "        'Prepare Metadata - subjects - Existing - Local',\n",
    "        'Prepare Metadata - subjects - Existing - Pennsieve',\n",
    "        'Prepare Metadata - readme',\n",
    "        'Prepare Metadata - readme - Generate',\n",
    "        'Prepare Metadata - readme - Generate - Local',\n",
    "        'Prepare Metadata - readme - Generate - Pennsieve',\n",
    "        'Prepare Metadata - readme - Existing',\n",
    "        'Prepare Metadata - readme - Existing - Local',\n",
    "        'Prepare Metadata - readme - Existing - Pennsieve',\n",
    "        'Prepare Metadata - changes',\n",
    "        'Prepare Metadata - changes - Generate',\n",
    "        'Prepare Metadata - changes - Generate - Local',\n",
    "        'Prepare Metadata - changes - Generate - Pennsieve',\n",
    "        'Prepare Metadata - changes - Existing',\n",
    "        'Prepare Metadata - changes - Existing - Local',\n",
    "        'Prepare Metadata - changes - Existing - Pennsieve',\n",
    "        'Prepare Metadata - manifest',\n",
    "        'Prepare Metadata - manifest - Generate',\n",
    "        'Prepare Metadata - manifest - Generate - Local',\n",
    "        'Prepare Metadata - manifest - Generate - Pennsieve',\n",
    "        'Prepare Metadata - manifest - Existing',\n",
    "        'Prepare Metadata - manifest - Existing - Local',\n",
    "        'Prepare Metadata - manifest - Existing - Pennsieve',\n",
    "        \n",
    "        \n",
    "        'Download Template - manifest.xlsx',\n",
    "        'Download Template - manifest.xlsx',\n",
    "        'Download Template - dataset_description.xlsx',\n",
    "        'Download Template - subjects.xlsx',\n",
    "        'Download Template - samples.xlsx',\n",
    "        'Download Template - submission.xlsx',\n",
    "        \n",
    "        \n",
    "        'Prepare Datasets - Organize dataset',\n",
    "        'Prepare Datasets - Organize dataset - Existing',\n",
    "        'Prepare Datasets - Organize dataset - Existing - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Existing - Local',\n",
    "        'Prepare Datasets - Organize dataset - Existing - Saved',\n",
    "        \n",
    "        \n",
    "        'Prepare Datasets - Organize dataset - Step 3',\n",
    "        'Prepare Datasets - Organize dataset - Step 3 - Import',\n",
    "        'Prepare Datasets - Organize dataset - Step 3 - Import - File',\n",
    "        'Prepare Datasets - Organize dataset - Step 3 - Import - File - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 3 - Import - File - Saved',\n",
    "        'Prepare Datasets - Organize dataset - Step 3 - Import - File - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 3 - Import - File - New',\n",
    "        \n",
    "        'Prepare Datasets - Organize dataset - Step 3 - Import - Folder',\n",
    "        'Prepare Datasets - Organize dataset - Step 3 - Import - Folder - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 3 - Import - Folder - Saved',\n",
    "        'Prepare Datasets - Organize dataset - Step 3 - Import - Folder - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 3 - Import - Folder - New',\n",
    "        \n",
    "        'Prepare Datasets - Organize dataset - Step 3 - Add - Folder',\n",
    "        'Prepare Datasets - Organize dataset - Step 3 - Add - Folder - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 3 - Add - Folder - Saved',\n",
    "        'Prepare Datasets - Organize dataset - Step 3 - Add - Folder - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 3 - Add - Folder - New',\n",
    "        \n",
    "        \n",
    "        'Prepare Datasets - Organize dataset - Step 4',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - subjects - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - subjects - Local - Saved',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - subjects - Local - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - subjects - Local - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - subjects - Local - New',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - subjects - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - subjects - Pennsieve - Saved',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - subjects - Pennsieve - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - subjects - Pennsieve - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - subjects - Pennsieve - New',\n",
    "        \n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - samples - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - samples - Local - Saved',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - samples - Local - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - samples - Local - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - samples - Local - New',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - samples - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - samples - Pennsieve - Saved',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - samples - Pennsieve - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - samples - Pennsieve - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - samples - Pennsieve - New',\n",
    "        \n",
    "        \n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - submission - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - submission - Local - Saved',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - submission - Local - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - submission - Local - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - submission - Local - New',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - submission - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - submission - Pennsieve - Saved',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - submission - Pennsieve - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - submission - Pennsieve - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - submission - Pennsieve - New',\n",
    "        \n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - dataset_description - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - dataset_description - Local - Saved',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - dataset_description - Local - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - dataset_description - Local - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - dataset_description - Local - New',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - dataset_description - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - dataset_description - Pennsieve - Saved',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - dataset_description - Pennsieve - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - dataset_description - Pennsieve - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - dataset_description - Pennsieve - New',\n",
    "        \n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - README - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - README - Local - Saved',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - README - Local - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - README - Local - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - README - Local - New',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - README - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - README - Pennsieve - Saved',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - README - Pennsieve - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - README - Pennsieve - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - README - Pennsieve - New',\n",
    "        \n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - CHANGES - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - CHANGES - Local - Saved',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - CHANGES - Local - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - CHANGES - Local - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - CHANGES - Local - New',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - CHANGES - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - CHANGES - Pennsieve - Saved',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - CHANGES - Pennsieve - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - CHANGES - Pennsieve - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - CHANGES - Pennsieve - New',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - CHANGES - Pennsieve',\n",
    "        \n",
    "        \n",
    "        'Prepare Datasets - Organize dataset - Step 7',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Pennsieve - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Pennsieve - Saved',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Pennsieve - New',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Pennsieve - Pennsieve',\n",
    "        \n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Local - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Local - Saved',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Local - New',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Local - Pennsieve',\n",
    "        \n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Size',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Pennsieve - Size',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Local - Size',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Number of Files',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Pennsieve - Number of Files',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Local - Number of Files',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Manifest',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Manifest - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Manifest - Local',\n",
    "        \n",
    "        \n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Pennsieve - Create a duplicate',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Pennsieve - Replace',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Pennsieve - Merge',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Pennsieve - Skip',\n",
    "        \n",
    "        'Disseminate Datasets - Show current dataset permission',\n",
    "        'Disseminate Datasets - Show current dataset status',\n",
    "        'Disseminate Datasets - Pre-publishing Review - Integrate ORCID iD',\n",
    "        'Disseminate Datasets - Pre-publishing Review - Get Excluded Files',\n",
    "        'Disseminate Datasets - Pre-publishing Review - Get Metadata Files',\n",
    "        'Disseminate Datasets - Pre-publishing Review - Update excluded files',\n",
    "        'Disseminate Datasets - Pre-publishing Review - Publish',\n",
    "        'Disseminate Datasets - Pre-publishing Review - Submit dataset',\n",
    "        'Disseminate Datasets - Pre-publishing Review - Withdraw dataset',\n",
    "        'Disseminate Datasets - Pre-publishing Review - Fetch Pre-publishing Checklist Statuses',\n",
    "        \"Disseminate Datasets - Pre-publishing Review - Determine User's Dataset Role\",\n",
    "        \"Disseminate Datasets - Pre-publishing Review - Show publishing status\"\n",
    "        \n",
    "        'Disseminate Datasets - Share with Curation Team',\n",
    "        \"Disseminate Datasets - Share with Curation Team - Remove Consortium's Team Permissions\",\n",
    "        \"Disseminate Datasets - Share with Curation Team - Give Consortium Team Permissions\",\n",
    "        \"Disseminate Datasets - Share with Curation Team - Change Dataset Status to Work In Progress\",\n",
    "        \"Disseminate Datasets - Share with Curation Team - Change Dataset Status to Ready for Curation\",\n",
    "        \n",
    "        'Disseminate Datasets - Share with Consortium',\n",
    "        'Disseminate Datasets - Share with Consortium - Removed Team Permissions SPARC Consortium',\n",
    "        'Disseminate Datasets - Share with Consortium - Add Team Permissions SPARC Consortium',\n",
    "        'Disseminate Datasets - Share with Consortium - Curated & Awaiting PI Approval',\n",
    "        'Disseminate Datasets - Share with Consortium - Change Dataset Status to Under Embargo'\n",
    "        'Disseminate Datasets - Pre-publishing Review',\n",
    "        ],\n",
    "    value='New/Returning Users',\n",
    "    description='Option:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "start_date = widgets.DatePicker(description='Start Date:', disabled=False)\n",
    "end_date = widgets.DatePicker(description='End Date:', disabled=False)\n",
    "\n",
    "update_interval = widgets.Dropdown(options=['Daily', 'Weekly', 'Monthly', \"No Separation\"], description='Update Interval:', disabled=False)\n",
    "\n",
    "display(feature, start_date, end_date, update_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The cell below is a basic function that uses the widgets in the cell above to create a graph. If the widgets are not showing, run the widget cell. You don't have to run it again after selecting a value. Changing the value of the dropdown will dynamically change the value of the variable in the next cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = start_date.value\n",
    "ds = end_date.value\n",
    "\n",
    "data, new_user_data, returning_user_data = [], [], []\n",
    "column_headers = []\n",
    "file_name = \"\"\n",
    "bar_counter = 0\n",
    "\n",
    "if update_interval.value == \"Daily\":\n",
    "    bar_counter = progress_bar_counter(dt, ds, \"Daily\")\n",
    "    start = end = dt\n",
    "    column_headers = ['Day', 'Frequency']\n",
    "    file_name = \"daily\"\n",
    "if update_interval.value == \"Weekly\":\n",
    "    bar_counter = progress_bar_counter(dt, ds, \"Weekly\")    \n",
    "    start = dt - timedelta(days=dt.weekday())\n",
    "    end = start + timedelta(days=6)\n",
    "    column_headers = ['Week', 'Frequency']\n",
    "    file_name = \"weekly\"\n",
    "if update_interval.value == \"Monthly\":\n",
    "    bar_counter = progress_bar_counter(dt, ds, \"Monthly\")\n",
    "    start = end = dt\n",
    "    end = end.replace(day = calendar.monthrange(start.year, start.month)[1])\n",
    "    column_headers = ['Month', 'Frequency']\n",
    "    file_name = \"monthly\"\n",
    "if update_interval.value == \"No Separation\":\n",
    "    bar_counter = 1\n",
    "    start = dt\n",
    "    end = ds\n",
    "    column_headers = ['Time Period', 'Frequency']\n",
    "    file_name = \"no_Separation\"\n",
    "    \n",
    "for i in trange(bar_counter):\n",
    "    if start <= ds:\n",
    "        if feature.value == \"New/Returning Users\":\n",
    "            query = {\n",
    "                'reportRequests': [\n",
    "                {\n",
    "                    'viewId': VIEW_ID,\n",
    "                    'dateRanges': [{'startDate': start.strftime('%Y-%m-%d'), 'endDate': end.strftime('%Y-%m-%d')}],\n",
    "                    'metrics': [{'expression': 'ga:users'}],\n",
    "                    'dimensions': [{'name': 'ga:userType'}]\n",
    "                }]\n",
    "            }\n",
    "        else:\n",
    "            query = {\n",
    "                'reportRequests': [\n",
    "                {\n",
    "                    'viewId': VIEW_ID,\n",
    "                    'dateRanges': [{'startDate': start.strftime('%Y-%m-%d'), 'endDate': end.strftime('%Y-%m-%d')}],\n",
    "                    'metrics': [{'expression': 'ga:totalEvents'}],\n",
    "                    'dimensions': [{'name': 'ga:eventAction'}]\n",
    "                }]\n",
    "            }\n",
    "            \n",
    "        \n",
    "            \n",
    "        cell_data, new_user_cell_data, returning_user_cell_data = [], [], []\n",
    "        \n",
    "        if update_interval.value == \"Daily\":\n",
    "            cell_data_date = start.strftime(\"%d %b, %Y\")     \n",
    "        if update_interval.value == \"Weekly\" or update_interval.value == \"No Separation\":\n",
    "            cell_data_date = start.strftime(\"%d %b, %Y\") + \" - \" + end.strftime(\"%d %b, %Y\")    \n",
    "        if update_interval.value == \"Monthly\":\n",
    "            cell_data_date = start.strftime(\"%b %Y\")\n",
    "        \n",
    "        response = response_rows = []\n",
    "        response = get_report(analytics, query)\n",
    "        if \"rows\" in response[\"reports\"][0][\"data\"]:\n",
    "            response_rows = response[\"reports\"][0][\"data\"][\"rows\"]\n",
    "            \n",
    "        else:\n",
    "            response_rows = []\n",
    "            if feature.value == \"New/Returning Users\":\n",
    "                new_user_cell_data = [cell_data_date, 0]\n",
    "                new_user_data.append(new_user_cell_data)\n",
    "                returning_user_cell_data = [cell_data_date, 0]\n",
    "                returning_user_data.append(returning_user_cell_data)\n",
    "            else:\n",
    "                cell_data = [cell_data_date, 0]\n",
    "                data.append(cell_data)\n",
    "        \n",
    "        if feature.value == \"New/Returning Users\":\n",
    "            if response_rows != []:\n",
    "                new_user = False\n",
    "                returning_user = False\n",
    "                for res in response_rows:\n",
    "                    if (res[\"dimensions\"][0] == \"New Visitor\"):\n",
    "                        new_user_cell_data = [cell_data_date, int(res[\"metrics\"][0][\"values\"][0])]\n",
    "                        new_user_data.append(new_user_cell_data)\n",
    "                        new_user = True\n",
    "                    if (res[\"dimensions\"][0] == \"Returning Visitor\"):\n",
    "                        returning_user_cell_data = [cell_data_date, int(res[\"metrics\"][0][\"values\"][0])]\n",
    "                        returning_user_data.append(returning_user_cell_data)\n",
    "                        returning_user = True\n",
    "                if new_user == False:\n",
    "                    new_user_cell_data = [cell_data_date, 0]\n",
    "                    new_user_data.append(new_user_cell_data)\n",
    "                if returning_user == False:\n",
    "                    returning_user_cell_data = [cell_data_date, 0]\n",
    "                    returning_user_data.append(returning_user_cell_data)\n",
    "        else:\n",
    "            if response_rows != []:\n",
    "                response_present = False\n",
    "                for res in response_rows:\n",
    "                    if res[\"dimensions\"][0] == feature.value:\n",
    "                        cell_data = [cell_data_date, int(res[\"metrics\"][0][\"values\"][0])]\n",
    "                        data.append(cell_data)\n",
    "                        response_present = True\n",
    "                if response_present == False:\n",
    "                    cell_data = [cell_data_date, 0]\n",
    "                    data.append(cell_data)\n",
    "        \n",
    "        start, end = next_date_interval(start, end, update_interval.value)\n",
    "        \n",
    "folder_path = os.path.join(\"result_csv\", \"graph_data\")\n",
    "Path(folder_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df = new_df = returning_df = None\n",
    "if feature.value == \"New/Returning Users\":\n",
    "    \n",
    "    new_df = pd.DataFrame(new_user_data, columns = column_headers)\n",
    "    returning_df = pd.DataFrame(returning_user_data, columns = column_headers)\n",
    "    \n",
    "    new_action_column = new_df.iloc[:, 0]\n",
    "    new_frequency_column = new_df.iloc[:, 1]\n",
    "    new_x_markers = pd.Series(new_action_column).array\n",
    "    new_y_markers = pd.Series(new_frequency_column).array\n",
    "    new_y_markers = new_y_markers.astype(int)\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x = new_x_markers, y = new_y_markers, mode = 'lines', name = 'New Users'))\n",
    "    \n",
    "    ret_action_column = returning_df.iloc[:, 0]\n",
    "    ret_frequency_column = returning_df.iloc[:, 1]\n",
    "    ret_x_markers = pd.Series(ret_action_column).array\n",
    "    ret_y_markers = pd.Series(ret_frequency_column).array\n",
    "    ret_y_markers = ret_y_markers.astype(int)\n",
    "    \n",
    "    fig.add_trace(go.Scatter(x = ret_x_markers, y = ret_y_markers, mode = 'lines', name = 'Returning Users'))\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "else:\n",
    "    df = pd.DataFrame(data, columns = column_headers)\n",
    "    result_path = os.path.join(folder_path, file_name + \"_graph-\" + dt.strftime(\"%d %b, %Y\") + \" - \" + ds.strftime(\"%d %b, %Y\") + \".csv\")\n",
    "    df.to_csv(result_path, encoding='utf-8', index=False)    \n",
    "    df.astype({'Frequency': 'int32'}).dtypes\n",
    "    \n",
    "    fig = None\n",
    "\n",
    "    if update_interval.value == \"Daily\":\n",
    "        fig = px.line(df, x = \"Day\", y = \"Frequency\", render_mode = \"auto\", labels = {\"Day\": \"Date\",\"Frequency\": \"Frequency\"},\n",
    "            title = update_interval.value + \" Chart for '\" + feature.value + \"': \" + dt.strftime(\"%d %b, %Y\") + \" - \" + ds.strftime(\"%d %b, %Y\"))\n",
    "    if update_interval.value == \"Weekly\":\n",
    "        fig = px.line(df, x = \"Week\", y = \"Frequency\", render_mode = \"auto\", labels = {\"Day\": \"Week\",\"Frequency\": \"Frequency\"},\n",
    "            title = update_interval.value + \" Chart for '\" + feature.value + \"': \" + dt.strftime(\"%d %b, %Y\") + \" - \" + ds.strftime(\"%d %b, %Y\"))\n",
    "    if update_interval.value == \"Monthly\":\n",
    "        fig = px.line(df, x = \"Month\", y = \"Frequency\", render_mode = \"auto\", labels = {\"Day\": \"Month\",\"Frequency\": \"Frequency\"},\n",
    "            title = update_interval.value + \" Chart for '\" + feature.value + \"': \" + dt.strftime(\"%d %b, %Y\") + \" - \" + ds.strftime(\"%d %b, %Y\"))\n",
    "    if update_interval.value == \"No Separation\":\n",
    "        fig = px.scatter(df, x = \"Time Period\", y = \"Frequency\", render_mode = \"auto\", labels = {\"Day\": \"Time Period\",\"Frequency\": \"Frequency\"},\n",
    "            title = update_interval.value + \" Chart for '\" + feature.value + \"': \" + dt.strftime(\"%d %b, %Y\") + \" - \" + ds.strftime(\"%d %b, %Y\"))\n",
    "        fig.update_traces(marker={'size': 15})\n",
    "\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sunburst graph of actions excluding those in the ignore list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sunburst shows items within the date range selected in the date range UI elements\n",
    "start = start_date.value\n",
    "end = end_date.value\n",
    "category_dict = {\n",
    "    \"Manage Dataset\": {},\n",
    "    \"App\": {},\n",
    "    \"Disseminate Dataset\": {},\n",
    "    \"Generate Dataset\": {},\n",
    "    \"Prepare Metadata\": {},\n",
    "    \"Other\": {}\n",
    "}\n",
    "\n",
    "\n",
    "# to exclude items from the Sunburst graphic add their Action names here\n",
    "ignore_list = [\"Establishing Python Connection\", \n",
    "               \"App Launched - OS\", \n",
    "               \"App Launched - SODA\",\n",
    "               \"App Restarted\",\n",
    "               \"Update Downloaded\",\n",
    "               \"Update Requested\",\n",
    "              ]\n",
    "\n",
    "data = []\n",
    "\n",
    "query = {\n",
    "    'reportRequests': [\n",
    "    {\n",
    "        'viewId': VIEW_ID,\n",
    "        'dateRanges': [{'startDate': start.strftime('%Y-%m-%d'), 'endDate': end.strftime('%Y-%m-%d')}],\n",
    "        'metrics': [{'expression': 'ga:totalEvents'}],\n",
    "        'dimensions': [{'name': 'ga:eventAction'}]\n",
    "    }]\n",
    "}\n",
    "\n",
    "response = get_report(analytics, query)\n",
    "response_rows = response[\"reports\"][0][\"data\"][\"rows\"]\n",
    "\n",
    "for res in response_rows:\n",
    "    response_action = res[\"dimensions\"][0]\n",
    "    response_value = res[\"metrics\"][0][\"values\"][0]\n",
    "    \n",
    "    if(response_action in ignore_list):\n",
    "        continue\n",
    "    \n",
    "    action_found = False\n",
    "    for key in category_dict:\n",
    "        if (response_action.find(key) != -1 ):\n",
    "            if response_action in category_dict[key]:\n",
    "                category_dict[key][response_action] += response_value\n",
    "            else:\n",
    "                category_dict[key][response_action] = response_value\n",
    "            action_found = True\n",
    "            \n",
    "    if action_found == False:\n",
    "        if response_action.find(\"Manifest Files Created\") != -1:\n",
    "            if response_action in category_dict[\"Generate Dataset\"]:\n",
    "                category_dict[\"Generate Dataset\"][response_action] += response_value\n",
    "            else:\n",
    "                category_dict[\"Generate Dataset\"][response_action] = response_value\n",
    "        elif response_action in category_dict[\"Other\"]:\n",
    "            category_dict[\"Other\"][response_action] += response_value\n",
    "        else:\n",
    "            category_dict[\"Other\"][response_action] = response_value\n",
    "    \n",
    "for key in category_dict:\n",
    "    for action_key in category_dict[key]:\n",
    "        cell_data = [key, action_key, category_dict[key][action_key]]\n",
    "        data.append(cell_data)\n",
    "    \n",
    "df = pd.DataFrame(data, columns = [\"Action\", \"Subaction\", \"Total\"])\n",
    "result_path = os.path.join(\"test.csv\")\n",
    "df.to_csv(result_path, encoding='utf-8', index=False)\n",
    "\n",
    "fig = px.sunburst(df, path=['Action', 'Subaction'], values='Total',\n",
    "                  color='Subaction', hover_data=['Total'])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set start and end date for all further reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the dropdown values here\n",
    "dt = start_date.value\n",
    "ds = end_date.value\n",
    "\n",
    "# Date format in 'YYYY-MM-DD'\n",
    "# You can also use relative dates for simplicity\n",
    "# start_date = \"50daysAgo\"\n",
    "# end_date = \"today\"\n",
    "# end_date = \"yesterday\"\n",
    "# start_date = \"2021-01-23\"\n",
    "# end_date = \"2021-04-23\"\n",
    "\n",
    "# Comment this out to use the  regular format dates above \n",
    "start = dt.strftime('%Y-%m-%d')\n",
    "end = ds.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset ID to dataset name mapping for all datasets (used in some of the reports below run first) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'N:dataset:ee8856a2-9789-4965-9619-f5d83ab08830': 'charmander-zard'}\n"
     ]
    }
   ],
   "source": [
    "# stores a Set of datasetId -> name pairs\n",
    "# IMP: mapping only exists from January 10th, 2022 onward\n",
    "# Gets populated by the function 'createDatasetIdToNameMapping'\n",
    "# Datasets being in this Set does not gurantee they have been touched/processed;\n",
    "# but simply that they have been selected by a SODA user.\n",
    "idNameMap = {}\n",
    "\n",
    "\n",
    "\n",
    "# The below function can result in duplicates when: \n",
    "# Multiple name changes occur during the same moment in time, where a moment is the Year, Month, Day, Hour, Minute. \n",
    "# Why: Google Analytics will sort dataset names alphabetically within a single time frame/moment. If a dataset \n",
    "#      changes names twice within that timeframe then the alphabetically greater name will be selected as opposed \n",
    "#      to the chronologically greater name. This means another datasetId can also be mapped to that name given the below code\n",
    "#      should they vacate the name within the same minute it was vacated by another datasetId/user in SODA.\n",
    "# That said this is unlikely, especially as if the user changes their dataset name after this occurs in a separate moment\n",
    "# then their mapping will not result in this situation as we take that most recent mapping from Analytics first then stop using\n",
    "# their datasetId.\n",
    "def createDatasetIdToNameMapping(start, end):\n",
    "    query = {\n",
    "    'reportRequests': [\n",
    "    {\n",
    "        'viewId': VIEW_ID,\n",
    "        'dateRanges': [{'startDate': start, 'endDate': end}],\n",
    "        'dimensions': [{'name': 'ga:eventCategory'}, {'name': 'ga:eventAction'}, {'name': 'ga:eventLabel'}, \n",
    "                       {'name': 'ga:date'}, {'name': 'ga:hour'}, {'name': 'ga:minute'} ],\n",
    "        'orderBys': [\n",
    "            {\n",
    "                 \"fieldName\": \"ga:date\", \n",
    "                 \"sortOrder\": \"DESCENDING\"\n",
    "            },\n",
    "            {\n",
    "                \"fieldName\": \"ga:hour\",\n",
    "                \"sortOrder\": \"DESCENDING\"\n",
    "            },\n",
    "            {\n",
    "                \"fieldName\": \"ga:minute\",\n",
    "                \"sortOrder\": \"DESCENDING\"\n",
    "            }\n",
    "        ]\n",
    "    }]\n",
    "    }\n",
    "    \n",
    "    visited_ids = {}\n",
    "    response = get_report(analytics, query)\n",
    "    response_rows = response[\"reports\"][0][\"data\"][\"rows\"]\n",
    "    \n",
    "    for res in response_rows:\n",
    "        \n",
    "        # check if the res category is \"Dataset ID to Dataset Name Map\"\n",
    "        if res[\"dimensions\"][0] == \"Dataset ID to Dataset Name Map\":\n",
    "            lb = res[\"dimensions\"][1]\n",
    "            \n",
    "            # check if this datasetId has been visited before \n",
    "            if lb in visited_ids.keys():\n",
    "                # if so do not process the id to name\n",
    "                continue\n",
    "                \n",
    "            # mark the id as having been visited \n",
    "            visited_ids[lb] = True\n",
    "                \n",
    "            # print(res)\n",
    "            # get the action (the datasetId)\n",
    "            did = res[\"dimensions\"][1]\n",
    "            \n",
    "            # get the label (dataset name)\n",
    "            dname = res[\"dimensions\"][2]\n",
    "        \n",
    "            # assign the action to the label (dataset name) in the idNameMap \n",
    "            idNameMap[did] = dname\n",
    "        \n",
    "    \n",
    " \n",
    "createDatasetIdToNameMapping(start, end)\n",
    "print(idNameMap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a list of all datasets for which the actions below have been done on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all of the actions that will be used when considering if a dataset has been touched/processed\n",
    "# add or remove tracked Actions at will\n",
    "all_actions = [\n",
    "        'Manage Datasets - Create a new dataset',\n",
    "        'Manage Datasets - Rename an existing dataset',\n",
    "        'Manage Datasets - Make PI owner of dataset',\n",
    "        'Manage Datasets - Add/Edit Permissions',\n",
    "        'Manage Datasets - Add/Edit Permissions - Add User Permissions',\n",
    "        'Manage Datasets - Add/Edit Permissions - Add Team Permissions',\n",
    "        \"Manage Datasets - Add/Edit Subtitle\",\n",
    "        \"Manage Datasets - Add/Edit Subtitle - Get Subtitle\",\n",
    "        \"Manage Datasets - Add/Edit Readme\",\n",
    "        \"Manage Datasets - Add/Edit Readme - Get Readme\",\n",
    "        \"Manage Datasets - Add/Edit Readme - Parse Readme\",  \n",
    "        \"Manage Datasets - Upload a Banner Image\",\n",
    "        \"Manage Datasets - Upload a Banner Image - Size\",\n",
    "        \"Manage Datasets - Upload a Banner Image - Importing Banner Image\",\n",
    "        \"Manage Datasets - Upload a Banner Image - Get Banner Image\",\n",
    "        \"Manage Datasets - Add/Edit Tags\",\n",
    "        \"Manage Datasets - Add/Edit Tags - Get Tags\",\n",
    "        \"Manage Datasets - Assign a License\",\n",
    "        \"Manage Datasets - Assign a License - Get License\",\n",
    "        \"Manage Datasets - Upload Local Dataset\",\n",
    "        \"Manage Datasets - Upload Local Dataset - size\",\n",
    "        \"Manage Datasets - Upload Local Dataset - name - size\",\n",
    "        \"Manage Datasets - Upload Local Dataset - Number of Folders\",\n",
    "        \"Manage Datasets - Upload Local Dataset - name - Number of Folders\",\n",
    "        \"Manage Datasets - Upload Local Dataset - Number of Files\",\n",
    "        \"Manage Datasets - Upload Local Dataset - name - Number of Files\",\n",
    "        \"Manage Datasets - Change Dataset Status\",\n",
    "        \"Manage Datasets - Change Dataset Status - Get Dataset Status\"\n",
    "        \n",
    "        \n",
    "        \n",
    "        'Prepare Metadata - Add Airtable account',\n",
    "        'Prepare Metadata - Add DDD',\n",
    "        'Prepare Metadata - Create Submission',\n",
    "        'Prepare Metadata - Create dataset_description',\n",
    "        'Prepare Metadata - samples',\n",
    "        'Prepare Metadata - samples - Generate',\n",
    "        'Prepare Metadata - samples - Generate - Local',\n",
    "        'Prepare Metadata - samples - Generate - Pennsieve',\n",
    "        'Prepare Metadata - samples - Existing',\n",
    "        'Prepare Metadata - samples - Existing - Local',\n",
    "        'Prepare Metadata - samples - Existing - Pennsieve',\n",
    "        'Prepare Metadata - submission',\n",
    "        'Prepare Metadata - submission - Generate',\n",
    "        'Prepare Metadata - submission - Generate - Local',\n",
    "        'Prepare Metadata - submission - Generate - Pennsieve',\n",
    "        'Prepare Metadata - submission - Existing',\n",
    "        'Prepare Metadata - submission - Existing - Local',\n",
    "        'Prepare Metadata - submission - Existing - Pennsieve',\n",
    "        'Prepare Metadata - dataset_description',\n",
    "        'Prepare Metadata - dataset_description - Generate',\n",
    "        'Prepare Metadata - dataset_description - Generate - Local',\n",
    "        'Prepare Metadata - dataset_description - Generate - Pennsieve',\n",
    "        'Prepare Metadata - dataset_description - Existing',\n",
    "        'Prepare Metadata - dataset_description - Existing - Local',\n",
    "        'Prepare Metadata - dataset_description - Existing - Pennsieve',\n",
    "        'Prepare Metadata - subjects',\n",
    "        'Prepare Metadata - subjects - Generate',\n",
    "        'Prepare Metadata - subjects - Generate - Local',\n",
    "        'Prepare Metadata - subjects - Generate - Pennsieve',\n",
    "        'Prepare Metadata - subjects - Existing',\n",
    "        'Prepare Metadata - subjects - Existing - Local',\n",
    "        'Prepare Metadata - subjects - Existing - Pennsieve',\n",
    "        'Prepare Metadata - readme',\n",
    "        'Prepare Metadata - readme - Generate',\n",
    "        'Prepare Metadata - readme - Generate - Local',\n",
    "        'Prepare Metadata - readme - Generate - Pennsieve',\n",
    "        'Prepare Metadata - readme - Existing',\n",
    "        'Prepare Metadata - readme - Existing - Local',\n",
    "        'Prepare Metadata - readme - Existing - Pennsieve',\n",
    "        'Prepare Metadata - changes',\n",
    "        'Prepare Metadata - changes - Generate',\n",
    "        'Prepare Metadata - changes - Generate - Local',\n",
    "        'Prepare Metadata - changes - Generate - Pennsieve',\n",
    "        'Prepare Metadata - changes - Existing',\n",
    "        'Prepare Metadata - changes - Existing - Local',\n",
    "        'Prepare Metadata - changes - Existing - Pennsieve',\n",
    "        'Prepare Metadata - manifest',\n",
    "        'Prepare Metadata - manifest - Generate',\n",
    "        'Prepare Metadata - manifest - Generate - Local',\n",
    "        'Prepare Metadata - manifest - Generate - Pennsieve',\n",
    "        'Prepare Metadata - manifest - Existing',\n",
    "        'Prepare Metadata - manifest - Existing - Local',\n",
    "        'Prepare Metadata - manifest - Existing - Pennsieve',\n",
    "        \n",
    "        \n",
    "        'Download Template - manifest.xlsx',\n",
    "        'Download Template - manifest.xlsx',\n",
    "        'Download Template - dataset_description.xlsx',\n",
    "        'Download Template - subjects.xlsx',\n",
    "        'Download Template - samples.xlsx',\n",
    "        'Download Template - submission.xlsx',\n",
    "        \n",
    "        \n",
    "        'Prepare Datasets - Organize dataset',\n",
    "        'Prepare Datasets - Organize dataset - Existing',\n",
    "        'Prepare Datasets - Organize dataset - Existing - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Existing - Local',\n",
    "        'Prepare Datasets - Organize dataset - Existing - Saved',\n",
    "        \n",
    "        \n",
    "        'Prepare Datasets - Organize dataset - Step 3',\n",
    "        'Prepare Datasets - Organize dataset - Step 3 - Import',\n",
    "        'Prepare Datasets - Organize dataset - Step 3 - Import - File',\n",
    "        'Prepare Datasets - Organize dataset - Step 3 - Import - File - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 3 - Import - File - Saved',\n",
    "        'Prepare Datasets - Organize dataset - Step 3 - Import - File - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 3 - Import - File - New',\n",
    "        \n",
    "        'Prepare Datasets - Organize dataset - Step 3 - Import - Folder',\n",
    "        'Prepare Datasets - Organize dataset - Step 3 - Import - Folder - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 3 - Import - Folder - Saved',\n",
    "        'Prepare Datasets - Organize dataset - Step 3 - Import - Folder - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 3 - Import - Folder - New',\n",
    "        \n",
    "        'Prepare Datasets - Organize dataset - Step 3 - Add - Folder',\n",
    "        'Prepare Datasets - Organize dataset - Step 3 - Add - Folder - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 3 - Add - Folder - Saved',\n",
    "        'Prepare Datasets - Organize dataset - Step 3 - Add - Folder - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 3 - Add - Folder - New',\n",
    "        \n",
    "        \n",
    "        'Prepare Datasets - Organize dataset - Step 4',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - subjects - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - subjects - Local - Saved',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - subjects - Local - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - subjects - Local - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - subjects - Local - New',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - subjects - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - subjects - Pennsieve - Saved',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - subjects - Pennsieve - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - subjects - Pennsieve - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - subjects - Pennsieve - New',\n",
    "        \n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - samples - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - samples - Local - Saved',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - samples - Local - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - samples - Local - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - samples - Local - New',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - samples - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - samples - Pennsieve - Saved',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - samples - Pennsieve - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - samples - Pennsieve - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - samples - Pennsieve - New',\n",
    "        \n",
    "        \n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - submission - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - submission - Local - Saved',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - submission - Local - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - submission - Local - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - submission - Local - New',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - submission - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - submission - Pennsieve - Saved',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - submission - Pennsieve - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - submission - Pennsieve - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - submission - Pennsieve - New',\n",
    "        \n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - dataset_description - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - dataset_description - Local - Saved',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - dataset_description - Local - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - dataset_description - Local - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - dataset_description - Local - New',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - dataset_description - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - dataset_description - Pennsieve - Saved',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - dataset_description - Pennsieve - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - dataset_description - Pennsieve - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - dataset_description - Pennsieve - New',\n",
    "        \n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - README - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - README - Local - Saved',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - README - Local - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - README - Local - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - README - Local - New',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - README - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - README - Pennsieve - Saved',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - README - Pennsieve - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - README - Pennsieve - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - README - Pennsieve - New',\n",
    "        \n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - CHANGES - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - CHANGES - Local - Saved',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - CHANGES - Local - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - CHANGES - Local - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - CHANGES - Local - New',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - CHANGES - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - CHANGES - Pennsieve - Saved',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - CHANGES - Pennsieve - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - CHANGES - Pennsieve - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - CHANGES - Pennsieve - New',\n",
    "        'Prepare Datasets - Organize dataset - Step 4 - Import - CHANGES - Pennsieve',\n",
    "        \n",
    "        \n",
    "        'Prepare Datasets - Organize dataset - Step 7',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Pennsieve - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Pennsieve - Saved',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Pennsieve - New',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Pennsieve - Pennsieve',\n",
    "        \n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Local - Local',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Local - Saved',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Local - New',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Local - Pennsieve',\n",
    "        \n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Size',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Pennsieve - Size',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Local - Size',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Number of Files',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Pennsieve - Number of Files',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Local - Number of Files',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Manifest',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Manifest - Pennsieve',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Manifest - Local',\n",
    "        \n",
    "        \n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Pennsieve - Create a duplicate',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Pennsieve - Replace',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Pennsieve - Merge',\n",
    "        'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Pennsieve - Skip',\n",
    "        \n",
    "        'Disseminate Datasets - Show current dataset permission',\n",
    "        'Disseminate Datasets - Show current dataset status',\n",
    "        'Disseminate Datasets - Pre-publishing Review - Integrate ORCID iD',\n",
    "        'Disseminate Datasets - Pre-publishing Review - Get Excluded Files',\n",
    "        'Disseminate Datasets - Pre-publishing Review - Get Metadata Files',\n",
    "        'Disseminate Datasets - Pre-publishing Review - Update excluded files',\n",
    "        'Disseminate Datasets - Pre-publishing Review - Publish',\n",
    "        'Disseminate Datasets - Pre-publishing Review - Submit dataset',\n",
    "        'Disseminate Datasets - Pre-publishing Review - Withdraw dataset',\n",
    "        'Disseminate Datasets - Pre-publishing Review - Fetch Pre-publishing Checklist Statuses',\n",
    "        \"Disseminate Datasets - Pre-publishing Review - Determine User's Dataset Role\",\n",
    "        \"Disseminate Datasets - Pre-publishing Review - Show publishing status\"\n",
    "        \n",
    "        'Disseminate Datasets - Share with Curation Team',\n",
    "        \"Disseminate Datasets - Share with Curation Team - Remove Consortium's Team Permissions\",\n",
    "        \"Disseminate Datasets - Share with Curation Team - Give Consortium Team Permissions\",\n",
    "        \"Disseminate Datasets - Share with Curation Team - Change Dataset Status to Work In Progress\",\n",
    "        \"Disseminate Datasets - Share with Curation Team - Change Dataset Status to Ready for Curation\",\n",
    "        \n",
    "        'Disseminate Datasets - Share with Consortium',\n",
    "        'Disseminate Datasets - Share with Consortium - Removed Team Permissions SPARC Consortium',\n",
    "        'Disseminate Datasets - Share with Consortium - Add Team Permissions SPARC Consortium',\n",
    "        'Disseminate Datasets - Share with Consortium - Curated & Awaiting PI Approval',\n",
    "        'Disseminate Datasets - Share with Consortium - Change Dataset Status to Under Embargo'\n",
    "        'Disseminate Datasets - Pre-publishing Review',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 ||||| ['charmander-zard', '(not set)', 'hshshsh']\n"
     ]
    }
   ],
   "source": [
    "def is_valid_uuid(value):\n",
    "    try:\n",
    "        uuid.UUID(value)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "\n",
    "dataset_list = []\n",
    "\n",
    "# tracks local datasets that have been processed through SODA\n",
    "# does not remove duplicates\n",
    "dataset_list_local = []\n",
    "\n",
    "processed_dataset_id_to_name_list = {}\n",
    "\n",
    "def datasets_and_actions_update(start, end):\n",
    "    query = {\n",
    "        'reportRequests': [\n",
    "        {\n",
    "            'viewId': VIEW_ID,\n",
    "            'dateRanges': [{'startDate': start, 'endDate': end}],\n",
    "            'metrics': [{'expression': 'ga:totalEvents'}],\n",
    "            'dimensions': [{'name': 'ga:eventCategory'}, {'name': 'ga:eventAction'}, {'name': 'ga:eventLabel'}, \n",
    "                           {'name': 'ga:Date'}, {'name': 'ga:hour'}, {'name': 'ga:minute'}],\n",
    "            # important for ensuring we get the latest name attached to a datasetID in the event of a user \n",
    "            # renaming their dataset.\n",
    "            # coupled with marking when a datasetId has been 'visited' we will always get the latest dataset names\n",
    "            # without including a single dataset's previous dataset name in the final list unless another dataset\n",
    "            # took that name after it was made available by a name change. This is mostly true except for when \n",
    "            # the key collisions (as outlined in the create dataset id to name mapping ) occur.\n",
    "            'orderBys': [\n",
    "            {\n",
    "                 \"fieldName\": \"ga:date\", \n",
    "                 \"sortOrder\": \"DESCENDING\"\n",
    "            },\n",
    "            {\n",
    "                \"fieldName\": \"ga:hour\",\n",
    "                \"sortOrder\": \"DESCENDING\"\n",
    "            },\n",
    "            {\n",
    "                \"fieldName\": \"ga:minute\",\n",
    "                \"sortOrder\": \"DESCENDING\"\n",
    "            }\n",
    "            ]\n",
    "        }]\n",
    "    }\n",
    "    response = get_report(analytics, query)\n",
    "    response_rows = response[\"reports\"][0][\"data\"][\"rows\"]\n",
    "    data = []\n",
    "    visited_dataset_id = {}\n",
    "\n",
    "    for res in response_rows:\n",
    "      \n",
    "        # only consider Actions that were successful \n",
    "        # if commented out unsuccessful user Actions will also be considered but the \n",
    "        # indentation will need to be adjusted\n",
    "        if res[\"dimensions\"][0] == \"Success\":\n",
    "            \n",
    "            lb = \"\"\n",
    "            \n",
    "            # renaming Actions have irregular formatting so handle parsing here\n",
    "            if res[\"dimensions\"][1] == \"Manage Datasets - Rename an existing dataset\":\n",
    "                # get the label value\n",
    "                temp_label = res[\"dimensions\"][2]\n",
    "                # strip the dataset id out of the label\n",
    "                # last occurrence of ':' - 1 indicates the end of an id\n",
    "                lb = temp_label[0:temp_label.rfind(\":\")]\n",
    "            else:\n",
    "                # the label is a name or datasetId\n",
    "                lb = res[\"dimensions\"][2]\n",
    "            \n",
    "           \n",
    "            # check if the label is a datasetId, stored as a UUID on Pennsieve\n",
    "            is_valid = is_valid_uuid(lb[10:])\n",
    "            \n",
    "                \n",
    "            # if a UUID convert to dataset name \n",
    "            if is_valid:\n",
    "                # get the dataset name\n",
    "                dsname = idNameMap[lb]\n",
    "                \n",
    "                # check if this datasetId has been visited before \n",
    "                if lb in visited_dataset_id.keys():\n",
    "                    # if so do not process the id to name\n",
    "                    continue\n",
    "                \n",
    "                # mark the id as having been visited \n",
    "                visited_dataset_id[lb] = True\n",
    "                \n",
    "                # add the file to the list of processed id->name pairs\n",
    "                processed_dataset_id_to_name_list[lb] = dsname\n",
    "                \n",
    "                # change the label to the dataset name \n",
    "                lb = dsname\n",
    "                \n",
    "                \n",
    "            # check if the Action indicates a Local dataset generated in the Organize Dataset step\n",
    "            if res[\"dimensions\"][1] == \"Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Local\" and res[\"dimensions\"][2] != \"Local\":\n",
    "                # IMP: When a local dataset is created in Organize section we log its name; therefore \n",
    "                #      to track those datasets there is a check for ... - Generate - Local to grab the name out of the label\n",
    "                #      and include it in the dataset list. This is the only kind of 'touched dataset' that will not\n",
    "                #      have a value in the idNameMap as of January 5th, 2022\n",
    "                dataset_list_local.append(lb)\n",
    "            \n",
    "                    \n",
    "                \n",
    "            # check if the label is a dataset name stored in the mapping - if we decide to track local datasets in Prepare \n",
    "            # Metadata then we should add them to the dataset_list_local otherwise they will get ignored at this step \n",
    "            if lb not in idNameMap.values():\n",
    "                # any dataset that has been worked on will have been selected; creating a dataset id to name mapping\n",
    "                # if the current label is not a value in that mapping then it is not a dataset name.\n",
    "                # One side effect is that any 'saved' datasets in the Organize datasets section will not be\n",
    "                # considered a touched dataset until they have been finalized by either being generated \n",
    "                # locally or on Pennsieve.\n",
    "                continue\n",
    "            \n",
    "            # do not include the dataset name if it is already in the list\n",
    "            if lb not in dataset_list:\n",
    "                dataset_list.append(lb)\n",
    "\n",
    "        \n",
    "    folder_path = os.path.join(\"result_csv\", \"custom\")\n",
    "    Path(folder_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "datasets_and_actions_update(start, end)\n",
    "\n",
    "finalized_dataset_list = [*dataset_list, *dataset_list_local]\n",
    "\n",
    "print(len(finalized_dataset_list), \"|||||\", finalized_dataset_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View all processed dataset ID-Name pairs available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ||||| ['N:dataset:ee8856a2-9789-4965-9619-f5d83ab08830: charmander-zard']\n"
     ]
    }
   ],
   "source": [
    "# mapping/Set of datasetId -> dataset name pairs that have been processed/touched in some way \n",
    "# that does not include dataset selection.\n",
    "# IMP: will not include local datasets as they do not have an ID\n",
    "# IMP: Run the above Cell to get meaningful results from this Cell\n",
    "processed_id_name_pairs = []\n",
    "for key in processed_dataset_id_to_name_list.keys():\n",
    "    processed_id_name_pairs.append(key + ': ' + processed_dataset_id_to_name_list[key])\n",
    "print(len(processed_id_name_pairs), \"|||||\", processed_id_name_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a report of all unique users within a given time frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is how to get total SODA users within a daterange\n",
    "query = {\n",
    "    'reportRequests': [\n",
    "    {\n",
    "        'viewId': VIEW_ID,\n",
    "        'dateRanges': [{'startDate': start, 'endDate': end}],\n",
    "        'metrics': [{'expression': 'ga:users'}],\n",
    "        'dimensions': [{'name': 'ga:userType'}]\n",
    "    }]\n",
    "}\n",
    "\n",
    "response = get_report(analytics, query)\n",
    "response_rows = response[\"reports\"][0][\"data\"][\"rows\"]\n",
    "data = []\n",
    "\n",
    "for res in response_rows:\n",
    "    cell_data = [res[\"dimensions\"][0], res[\"metrics\"][0][\"values\"][0]]\n",
    "    data.append(cell_data)\n",
    "    \n",
    "folder_path = os.path.join(\"result_csv\", \"users\")\n",
    "Path(folder_path).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "df = pd.DataFrame(data, columns = ['Type', 'Values'])\n",
    "result_path = os.path.join(folder_path, \"users-\" + start + \"_\" + end + \".csv\")\n",
    "df.to_csv(result_path, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a report of all new users within a given time frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {\n",
    "    'reportRequests': [\n",
    "    {\n",
    "        'viewId': VIEW_ID,\n",
    "        'dateRanges': [{'startDate': start, 'endDate': end}],\n",
    "        # use the metric for tracking new users within a date range\n",
    "        'metrics': [{'expression': 'ga:users'}, {'expression': 'ga:newUsers'}],\n",
    "        'dimensions': [{'name': 'ga:userType',}]\n",
    "    }]\n",
    "}\n",
    "\n",
    "response = get_report(analytics, query)\n",
    "response_rows = response[\"reports\"][0][\"data\"][\"rows\"]\n",
    "data = []\n",
    "\n",
    "for res in response_rows:\n",
    "    # get the new users from the reponses\n",
    "    if res[\"dimensions\"][0] == \"New Visitor\":\n",
    "        cell_data = [\"New Users\", res[\"metrics\"][0][\"values\"][1]]\n",
    "        data.append(cell_data)\n",
    "\n",
    "# place the list of new users in a csv titled new_users\n",
    "folder_path = os.path.join(\"result_csv\", \"new_users\")\n",
    "Path(folder_path).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "df = pd.DataFrame(data, columns = ['Type', 'Values'])\n",
    "result_path = os.path.join(folder_path, \"users-\" + start + \"_\" + end + \".csv\")\n",
    "df.to_csv(result_path, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the number of files and the size of all datasets that was uploaded through SODA for a given time frame 2022 Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_statistics(start, end):\n",
    "            \n",
    "    query = {\n",
    "        'reportRequests': [\n",
    "        {\n",
    "            'viewId': VIEW_ID,\n",
    "            'dateRanges': [{'startDate': start, 'endDate': end}],\n",
    "            'metrics': [{'expression': 'ga:uniqueEvents'}, {'expression': 'ga:eventValue'}, {'expression': 'ga:totalEvents'}],\n",
    "            'dimensions': [{'name': 'ga:eventCategory'}, {'name': 'ga:eventAction'}, \n",
    "                           {'name': 'ga:eventLabel'}],\n",
    "\n",
    "        }]\n",
    "    }\n",
    "    data = []\n",
    "    response = get_report(analytics, query)\n",
    "    response_rows = response[\"reports\"][0][\"data\"][\"rows\"]\n",
    "    \n",
    "    for res in response_rows:\n",
    "        # do not track Errors; comment out to receive Errors in the report\n",
    "        if res[\"dimensions\"][0] == \"Error\":\n",
    "            continue\n",
    "\n",
    "        # report the aggregate number of files for all local dataset uploads\n",
    "        if res[\"dimensions\"][1] == \"Manage Datasets - Upload Local Dataset - Number of Files\" and res[\"dimensions\"][2] == \"Number of files local dataset\":\n",
    "            value = int(res[\"metrics\"][0][\"values\"][1])\n",
    "            cell_data = [res[\"dimensions\"][0], value , res[\"dimensions\"][1]]\n",
    "            data.append(cell_data)\n",
    "        # report the aggregate size of all local dataset uploads\n",
    "        if res[\"dimensions\"][1] == \"Manage Datasets - Upload Local Dataset - size\" and res[\"dimensions\"][2] == \"Size\":\n",
    "            value =  int(res[\"metrics\"][0][\"values\"][1])\n",
    "            cell_data = [res[\"dimensions\"][0], res[\"dimensions\"][1] , value]\n",
    "            data.append(cell_data)\n",
    "            \n",
    "        \n",
    "        # report the aggregate number of files for all datasets generated in Organize datasets - both locally and on \n",
    "        # Pennsieve\n",
    "        if res[\"dimensions\"][1] == \"Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Number of Files\" and res[\"dimensions\"][2] == \"Number of Files\":\n",
    "            value =  int(res[\"metrics\"][0][\"values\"][1])\n",
    "            cell_data = [res[\"dimensions\"][0], value , res[\"dimensions\"][1]]\n",
    "            data.append(cell_data)\n",
    "        # report the aggregate size of the files uploaded in Organize datasets - both locally and on Pennsieve\n",
    "        if res[\"dimensions\"][1] == \"Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Size\" and res[\"dimensions\"][2] == \"Size\":\n",
    "            value = int(res[\"metrics\"][0][\"values\"][1])\n",
    "            cell_data = [res[\"dimensions\"][0], res[\"dimensions\"][1], value]\n",
    "            data.append(cell_data)\n",
    "        # number of manifest files created/processed when generating a dataset in the Organize dataset section\n",
    "        # occurs when a user wants to create additional manifest files for their dataset upon Generation\n",
    "        # existing manifest files are automatically included\n",
    "        if res[\"dimensions\"][1] == \"Prepare Datasets - Organize dataset - Step 7 - Generate - Manifest\":\n",
    "            value = int(res[\"metrics\"][0][\"values\"][1])\n",
    "            cell_data = [res[\"dimensions\"][0], value, res[\"dimensions\"][1]]\n",
    "            data.append(cell_data)\n",
    "            \n",
    "            \n",
    "        # get number of banner image files uploaded\n",
    "        if res[\"dimensions\"][1] == \"Manage Datasets - Upload a Banner Image\":\n",
    "            value = res[\"metrics\"][0][\"values\"][2]\n",
    "            cell_data = [res[\"dimensions\"][0], value, res[\"dimensions\"][1]]\n",
    "            data.append(cell_data)\n",
    "        # aggregate size of uploaded banner image files uploaded through SODA\n",
    "        if res[\"dimensions\"][1] == \"Manage Datasets - Upload a Banner Image - Size\" and res[\"dimensions\"][2] == \"Size\":\n",
    "            print(value)\n",
    "            value = int(res[\"metrics\"][0][\"values\"][1])\n",
    "            cell_data = [res[\"dimensions\"][0], res[\"dimensions\"][1], value]\n",
    "            data.append(cell_data)\n",
    "            \n",
    "            \n",
    "        # count amount of metadata files created -- equivalent to the amount of times a generate action was emitted\n",
    "        if res[\"dimensions\"][1] == \"Prepare Metadata - Generate\":\n",
    "            totalEvents = int(res[\"metrics\"][0][\"values\"][2])\n",
    "            cell_data = [res[\"dimensions\"][0], totalEvents, res[\"dimensions\"][1]]\n",
    "            data.append(cell_data)\n",
    "        # aggregate size of metadata files created through SODA\n",
    "        if res[\"dimensions\"][1] == \"Prepare Metadata - Generate\" and res[\"dimensions\"][2] == \"Size of Total Metadata Files Generated\":\n",
    "            value = int(res[\"metrics\"][0][\"values\"][1])\n",
    "            cell_data = [res[\"dimensions\"][0], res[\"dimensions\"][1], value]\n",
    "            data.append(cell_data)\n",
    "            \n",
    "            \n",
    "        # count amount of Manifest files created through\n",
    "        if res[\"dimensions\"][1] == \"Prepare Metadata - manifest - Generate - Number of Files\" and res[\"dimensions\"][2] == \"Number of Files\":\n",
    "            value = int(res[\"metrics\"][0][\"values\"][1])\n",
    "            cell_data = [res[\"dimensions\"][0], value, res[\"dimensions\"][1]]\n",
    "            data.append(cell_data)\n",
    "    \n",
    "    folder_path = os.path.join(\"result_csv\", \"custom\")\n",
    "    Path(folder_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    df = pd.DataFrame(data, columns = ['Status', 'Number of Files', 'Size in (bytes)'])\n",
    "    result_path = os.path.join(folder_path, \"dataset_statistics-update\" + start + \"_\" + end + \".csv\")\n",
    "    df.to_csv(result_path, encoding='utf-8', index=False)\n",
    "    return\n",
    "\n",
    "## useful for getting all details for upload to Pennsieve for a specific time period\n",
    "## all responses go to the custom folder\n",
    "# num_of_files_folders_in_dataset(start_date, end_date)\n",
    "dataset_statistics(start, end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the number of files and their size as of March xx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dimensions': ['Success', 'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Number of Files', '5332ea80-453a-41b4-b87a-f42bd296f467', '20220329', '09', '21'], 'metrics': [{'values': ['1', '47', '1']}]}\n",
      "{'dimensions': ['Success', 'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Size', '5332ea80-453a-41b4-b87a-f42bd296f467', '20220329', '09', '21'], 'metrics': [{'values': ['1', '1079979', '1']}]}\n",
      "{'dimensions': ['Success', 'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Number of Files', '999641b8-63e3-46a0-9b64-02cb1e7d7eaa', '20220329', '09', '19'], 'metrics': [{'values': ['1', '250', '1']}]}\n",
      "{'dimensions': ['Success', 'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Size', '999641b8-63e3-46a0-9b64-02cb1e7d7eaa', '20220329', '09', '19'], 'metrics': [{'values': ['1', '0', '1']}]}\n",
      "{'dimensions': ['Success', 'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Number of Files', 'cae67115-7100-4bbf-b46a-5eb681e61a70', '20220329', '09', '18'], 'metrics': [{'values': ['1', '9', '1']}]}\n",
      "{'dimensions': ['Success', 'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Size', 'cae67115-7100-4bbf-b46a-5eb681e61a70', '20220329', '09', '18'], 'metrics': [{'values': ['1', '2821229', '1']}]}\n",
      "{'dimensions': ['Success', 'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Number of Files', '1ab9c64b-a4c7-42bb-96e8-ffbd98f076bd', '20220329', '09', '08'], 'metrics': [{'values': ['0', '63', '3']}]}\n",
      "{'dimensions': ['Success', 'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Size', '1ab9c64b-a4c7-42bb-96e8-ffbd98f076bd', '20220329', '09', '08'], 'metrics': [{'values': ['0', '16960767', '3']}]}\n",
      "{'dimensions': ['Success', 'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Number of Files', '1ab9c64b-a4c7-42bb-96e8-ffbd98f076bd', '20220329', '09', '07'], 'metrics': [{'values': ['1', '9', '1']}]}\n",
      "{'dimensions': ['Success', 'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Size', '1ab9c64b-a4c7-42bb-96e8-ffbd98f076bd', '20220329', '09', '07'], 'metrics': [{'values': ['1', '5653589', '1']}]}\n",
      "{'dimensions': ['Success', 'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Number of Files', '746cf9e2-62e0-491f-a674-033597741f96', '20220329', '09', '00'], 'metrics': [{'values': ['0', '69', '2']}]}\n",
      "{'dimensions': ['Success', 'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Size', '746cf9e2-62e0-491f-a674-033597741f96', '20220329', '09', '00'], 'metrics': [{'values': ['0', '5653590', '2']}]}\n",
      "{'dimensions': ['Success', 'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Number of Files', '746cf9e2-62e0-491f-a674-033597741f96', '20220329', '08', '59'], 'metrics': [{'values': ['1', '24', '1']}]}\n",
      "{'dimensions': ['Success', 'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Size', '746cf9e2-62e0-491f-a674-033597741f96', '20220329', '08', '59'], 'metrics': [{'values': ['1', '1', '1']}]}\n",
      "{'dimensions': ['Success', 'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Number of Files', 'New', '20220329', '08', '56'], 'metrics': [{'values': ['0', '24', '1']}]}\n",
      "{'dimensions': ['Success', 'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Size', 'New', '20220329', '08', '56'], 'metrics': [{'values': ['0', '5653589', '1']}]}\n",
      "{'dimensions': ['Success', 'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Number of Files', 'New', '20220329', '08', '54'], 'metrics': [{'values': ['1', '12', '1']}]}\n",
      "{'dimensions': ['Success', 'Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Size', 'New', '20220329', '08', '54'], 'metrics': [{'values': ['1', '2826795', '1']}]}\n",
      "{'dimensions': ['Success', 'Manage Datasets - Upload Local Dataset - Number of Files', 'ac3556d2-fe27-408b-a3fd-1b61178fc08a', '20220329', '08', '49'], 'metrics': [{'values': ['1', '9', '1']}]}\n",
      "{'dimensions': ['Success', 'Manage Datasets - Upload Local Dataset - size', 'ac3556d2-fe27-408b-a3fd-1b61178fc08a', '20220329', '08', '49'], 'metrics': [{'values': ['1', '2821228', '1']}]}\n",
      "{'dimensions': ['Success', 'Manage Datasets - Upload Local Dataset - Number of Files', '99f4f38c-687d-492e-867c-66488f4c965d', '20220329', '08', '47'], 'metrics': [{'values': ['1', '15', '2']}]}\n",
      "{'dimensions': ['Success', 'Manage Datasets - Upload Local Dataset - size', '99f4f38c-687d-492e-867c-66488f4c965d', '20220329', '08', '47'], 'metrics': [{'values': ['1', '2521137', '2']}]}\n",
      "{'dimensions': ['Success', 'Manage Datasets - Upload Local Dataset - Number of Files', 'Number of files local dataset', '20220329', '08', '45'], 'metrics': [{'values': ['1', '12', '1']}]}\n",
      "{'dimensions': ['Success', 'Manage Datasets - Upload Local Dataset - size', 'Size', '20220329', '08', '45'], 'metrics': [{'values': ['1', '2826794', '1']}]}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "As of March 18th we are working on making how we track dataset uploads more robust. Both for Organize and \n",
    "Upload Local Datasets. When uploading to Pennsieve, there are two problems we needed to solve that informed this update: \n",
    "\n",
    "1. An upload can fail and cause the Zerorpc server to send back some error code. In these cases we need to have an approximately accurate \n",
    "   account of the amount of files that were successfully uploaded.\n",
    "2. An upload can freeze due to the Pennsieve Agent getting stuck on the step where it cleans its soft cache. In these cases we need to have already logged\n",
    "   the amount of files that have been uploaded; the user will simply close the app in these cases which never gives the front-end code a chance to log\n",
    "   anything at all. \n",
    "   \n",
    "For local datasets there is only the first problem. However, since now we alter how we track \"Number of Files\" for Pennsieve\n",
    "I will have to include Local, New, and Saved labels to get the data's whole story. This is better than making every upload\n",
    "get a UUID however; as the UUID's raison d'être is tracking data correctly even when the Pennsieve Agent freezes and local \n",
    "generation doesn't use the Pennsieve Agent. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def is_valid_uuid(value):\n",
    "    try:\n",
    "        uuid.UUID(value)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "def dataset_statistics_update(start, end):\n",
    "            \n",
    "    query = {\n",
    "        'reportRequests': [\n",
    "        {\n",
    "            'viewId': VIEW_ID,\n",
    "            'dateRanges': [{'startDate': start, 'endDate': end}],\n",
    "            'metrics': [{'expression': 'ga:uniqueEvents'}, {'expression': 'ga:eventValue'}, {'expression': 'ga:totalEvents'}],\n",
    "            'dimensions': [{'name': 'ga:eventCategory'}, {'name': 'ga:eventAction'}, {'name': 'ga:eventLabel'}, \n",
    "                           {'name': 'ga:Date'}, {'name': 'ga:hour'}, {'name': 'ga:minute'}],\n",
    "            'orderBys': [\n",
    "            {\n",
    "                 \"fieldName\": \"ga:date\", \n",
    "                 \"sortOrder\": \"DESCENDING\"\n",
    "            },\n",
    "            {\n",
    "                \"fieldName\": \"ga:hour\",\n",
    "                \"sortOrder\": \"DESCENDING\"\n",
    "            },\n",
    "            {\n",
    "                \"fieldName\": \"ga:minute\",\n",
    "                \"sortOrder\": \"DESCENDING\"\n",
    "            }\n",
    "            ]\n",
    "\n",
    "        }]\n",
    "    }\n",
    "    data = []\n",
    "    response = get_report(analytics, query)\n",
    "    response_rows = response[\"reports\"][0][\"data\"][\"rows\"]\n",
    "    \n",
    "    # track the UUIDs, their number of files and the size of the uploaded files in aggregate\n",
    "    session_id_to_file_properties = {}\n",
    "    \n",
    "    for res in response_rows:\n",
    "        # do not track Errors; comment out to receive Errors in the report\n",
    "        if res[\"dimensions\"][0] == \"Error\":\n",
    "            continue\n",
    "\n",
    "        # report the aggregate number of files for all local dataset uploads to Pennsieve\n",
    "        if res[\"dimensions\"][1] == \"Manage Datasets - Upload Local Dataset - Number of Files\":\n",
    "            print(res)\n",
    "            label = res[\"dimensions\"][2]\n",
    "            number_of_files = res[\"metrics\"][0][\"values\"][1]\n",
    "            \n",
    "            # check if this is analytics data from a Penssieve upload session \n",
    "            if is_valid_uuid(label):\n",
    "                # initialize the session values if not already done for the given session id \n",
    "                if not label in session_id_to_file_properties:\n",
    "                    # initialize for number of files and size uploaded for the given session\n",
    "                    # we can know if number of files or size has been accounted for a given session id\n",
    "                    # by checking if either value is no longer -1\n",
    "                    session_data = [-1, -1]\n",
    "                    session_id_to_file_properties[label] = session_data\n",
    "                \n",
    "                if label in session_id_to_file_properties:\n",
    "                    session_data = session_id_to_file_properties[label]\n",
    "                    # if number of files for this session data is < 0 this is the last log of the session \n",
    "                    # [since query orders by date in descending order, aka our target]\n",
    "                    if int(session_data[0]) < 0:\n",
    "                        # update the stored session data \n",
    "                        session_data[0] = number_of_files\n",
    "                        session_id_to_file_properties[label] = session_data\n",
    "                        \n",
    "                        # print the aggregate number of files uploaded during the given session to the results csv \n",
    "                        cell_data = [res[\"dimensions\"][0], number_of_files , res[\"dimensions\"][1]]\n",
    "                        data.append(cell_data)\n",
    "        \n",
    "        \n",
    "        # report the aggregate size of all local dataset uploads\n",
    "        if res[\"dimensions\"][1] == \"Manage Datasets - Upload Local Dataset - size\":\n",
    "            print(res)\n",
    "            size =  int(res[\"metrics\"][0][\"values\"][1])\n",
    "            label = res[\"dimensions\"][2]\n",
    "            \n",
    "            if is_valid_uuid(label):\n",
    "                if not label in session_id_to_file_properties:\n",
    "                    # initialize for number of files and size uploaded for the given session\n",
    "                    # we can know if number of files or size has been accounted for a given session id\n",
    "                    # by checking if either value is no longer -1\n",
    "                    session_data = [-1, -1]\n",
    "                    session_id_to_file_properties[label] = session_data\n",
    "                \n",
    "                \n",
    "                # only consider the final analytics log from any given session\n",
    "                if label in session_id_to_file_properties: \n",
    "                    session_data = session_id_to_file_properties[label]\n",
    "                    if int(session_data[1]) < 0:\n",
    "                        session_data[1] = size\n",
    "                        session_id_to_file_properties[label] = session_data\n",
    "                        # add it to the results csv \n",
    "                        cell_data = [res[\"dimensions\"][0], res[\"dimensions\"][1] , size]\n",
    "                        data.append(cell_data)\n",
    "            \n",
    "        \n",
    "        # report the aggregate number of files for all datasets generated in Organize datasets - both locally and on \n",
    "        # Pennsieve\n",
    "        if res[\"dimensions\"][1] == \"Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Number of Files\":\n",
    "            print(res)\n",
    "            # check if the label is a UUID \n",
    "            label = res[\"dimensions\"][2]\n",
    "            number_of_files = res[\"metrics\"][0][\"values\"][1]\n",
    "            \n",
    "            # check if this is analytics data from a Penssieve upload session \n",
    "            if is_valid_uuid(label):\n",
    "                if not label in session_id_to_file_properties:\n",
    "                    # initialize for number of files and size uploaded for the given session\n",
    "                    # we can know if number of files or size has been accounted for a given session id\n",
    "                    # by checking if either value is no longer -1\n",
    "                    session_data = [-1, -1]\n",
    "                    session_id_to_file_properties[label] = session_data\n",
    "                \n",
    "                # only consider the final analytics log from any given session\n",
    "                if label in session_id_to_file_properties:\n",
    "                    session_data = session_id_to_file_properties[label]\n",
    "                    if int(session_data[0]) < 0:\n",
    "                        session_data[0] = number_of_files \n",
    "                        session_id_to_file_properties[label] = session_data\n",
    "                        # add it to the results csv \n",
    "                        cell_data = [res[\"dimensions\"][0], number_of_files , res[\"dimensions\"][1]]\n",
    "                        data.append(cell_data)\n",
    "            else:       \n",
    "                # either local, saved, or new \n",
    "                # TODO: Test this approach\n",
    "                cell_data = [res[\"dimensions\"][0], number_of_files , res[\"dimensions\"][1]]\n",
    "                data.append(cell_data)\n",
    "                \n",
    "                \n",
    "        # report the aggregate size of the files uploaded in Organize datasets - both locally and on Pennsieve\n",
    "        if res[\"dimensions\"][1] == \"Prepare Datasets - Organize dataset - Step 7 - Generate - Dataset - Size\":\n",
    "            print(res)\n",
    "            label = res[\"dimensions\"][2]\n",
    "            size = int(res[\"metrics\"][0][\"values\"][1])\n",
    "            \n",
    "            if is_valid_uuid(label):\n",
    "                if not label in session_id_to_file_properties:\n",
    "                    # initialize for number of files and size uploaded for the given session\n",
    "                    # we can know if number of files or size has been accounted for a given session id\n",
    "                    # by checking if either value is no longer -1\n",
    "                    session_data = [-1, -1]\n",
    "                    session_id_to_file_properties[label] = session_data\n",
    "                \n",
    "                \n",
    "                # only consider the final analytics log from any given session\n",
    "                if label in session_id_to_file_properties:\n",
    "                    session_data = session_id_to_file_properties[label]\n",
    "                    if int(session_data[1]) < 0:\n",
    "                        # update the stored session data \n",
    "                        session_data[1] = size \n",
    "                        session_id_to_file_properties[label] = session_data\n",
    "                        \n",
    "                        # add it to the results csv \n",
    "                        cell_data = [res[\"dimensions\"][0], res[\"dimensions\"][1] , size]\n",
    "                        data.append(cell_data)\n",
    "            else:\n",
    "                # local, saved, new \n",
    "                cell_data = [res[\"dimensions\"][0], res[\"dimensions\"][1], size]\n",
    "                data.append(cell_data)\n",
    "            \n",
    "            \n",
    "            \n",
    "        # number of manifest files created/processed when generating a dataset in the Organize dataset section\n",
    "        # occurs when a user wants to create additional manifest files for their dataset upon Generation\n",
    "        # existing manifest files are automatically included\n",
    "        if res[\"dimensions\"][1] == \"Prepare Datasets - Organize dataset - Step 7 - Generate - Manifest\":\n",
    "            value = int(res[\"metrics\"][0][\"values\"][1])\n",
    "            cell_data = [res[\"dimensions\"][0], value, res[\"dimensions\"][1]]\n",
    "            data.append(cell_data)\n",
    "            \n",
    "            \n",
    "        # get number of banner image files uploaded\n",
    "        if res[\"dimensions\"][1] == \"Manage Datasets - Upload a Banner Image\":\n",
    "            value = res[\"metrics\"][0][\"values\"][2]\n",
    "            cell_data = [res[\"dimensions\"][0], value, res[\"dimensions\"][1]]\n",
    "            data.append(cell_data)\n",
    "        # aggregate size of uploaded banner image files uploaded through SODA\n",
    "        if res[\"dimensions\"][1] == \"Manage Datasets - Upload a Banner Image - Size\" and res[\"dimensions\"][2] == \"Size\":\n",
    "            print(value)\n",
    "            value = int(res[\"metrics\"][0][\"values\"][1])\n",
    "            cell_data = [res[\"dimensions\"][0], res[\"dimensions\"][1], value]\n",
    "            data.append(cell_data)\n",
    "            \n",
    "            \n",
    "        # count amount of metadata files created -- equivalent to the amount of times a generate action was emitted\n",
    "        if res[\"dimensions\"][1] == \"Prepare Metadata - Generate\":\n",
    "            totalEvents = int(res[\"metrics\"][0][\"values\"][2])\n",
    "            cell_data = [res[\"dimensions\"][0], totalEvents, res[\"dimensions\"][1]]\n",
    "            data.append(cell_data)\n",
    "        # aggregate size of metadata files created through SODA\n",
    "        if res[\"dimensions\"][1] == \"Prepare Metadata - Generate\" and res[\"dimensions\"][2] == \"Size of Total Metadata Files Generated\":\n",
    "            value = int(res[\"metrics\"][0][\"values\"][1])\n",
    "            cell_data = [res[\"dimensions\"][0], res[\"dimensions\"][1], value]\n",
    "            data.append(cell_data)\n",
    "            \n",
    "            \n",
    "        # count amount of Manifest files created through\n",
    "        if res[\"dimensions\"][1] == \"Prepare Metadata - manifest - Generate - Number of Files\" and res[\"dimensions\"][2] == \"Number of Files\":\n",
    "            value = int(res[\"metrics\"][0][\"values\"][1])\n",
    "            cell_data = [res[\"dimensions\"][0], value, res[\"dimensions\"][1]]\n",
    "            data.append(cell_data)\n",
    "    \n",
    "    folder_path = os.path.join(\"result_csv\", \"custom\")\n",
    "    Path(folder_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    df = pd.DataFrame(data, columns = ['Status', 'Number of Files', 'Size in (bytes)'])\n",
    "    result_path = os.path.join(folder_path, \"dataset_statistics-update\" + start + \"_\" + end + \".csv\")\n",
    "    df.to_csv(result_path, encoding='utf-8', index=False)\n",
    "    return\n",
    "\n",
    "## useful for getting all details for upload to Pennsieve for a specific time period\n",
    "## all responses go to the custom folder\n",
    "# num_of_files_folders_in_dataset(start_date, end_date)\n",
    "dataset_statistics_update(start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
